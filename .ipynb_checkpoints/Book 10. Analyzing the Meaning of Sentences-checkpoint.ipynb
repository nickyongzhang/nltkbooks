{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding\n",
    "## Querying a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Problem*\n",
    "\n",
    "a.\t\tWhich country is Athens in?\n",
    "\n",
    "b.\t\tGreece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1.1:\n",
    "\n",
    "city_table: A table of cities, countries and populations\n",
    "\n",
    "|City|Country|Population|\n",
    "|----|-------|----------|\n",
    "|athens|greece|1368|\n",
    "|bangkok|thailand|1178|\n",
    "|barcelona|spain|1280|\n",
    "|berlin|east_germany|3481|\n",
    "|birmingham|united_kingdom|1112|\n",
    "\n",
    "SQL `\tSELECT Country FROM city_table WHERE City = 'athens'` can easily get the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to translate English input to SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "S[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]\n",
      "VP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]\n",
      "VP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]\n",
      "NP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]\n",
      "PP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]\n",
      "AP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]\n",
      "NP[SEM='Country=\"greece\"'] -> 'Greece'\n",
      "NP[SEM='Country=\"china\"'] -> 'China'\n",
      "Det[SEM='SELECT'] -> 'Which' | 'What'\n",
      "N[SEM='City FROM city_table'] -> 'cities'\n",
      "IV[SEM=''] -> 'are'\n",
      "A[SEM=''] -> 'located'\n",
      "P[SEM=''] -> 'in'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT City FROM city_table WHERE Country=\"china\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/sql0.fcfg')\n",
    "query = 'What cities are located in China'\n",
    "trees = list(cp.parse(query.split()))\n",
    "answer = trees[0].label()['SEM']\n",
    "answer = [s for s in answer if s]\n",
    "q = ' '.join(answer)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin "
     ]
    }
   ],
   "source": [
    "from nltk.sem import chat80\n",
    "rows = chat80.sql_query('corpora/city_database/city.db', q)\n",
    "for r in rows: \n",
    "    print(r[0], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language, Semantics and Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, logic-based approaches to natural language semantics focus on those aspects of natural language which guide our judgments of consistency and inconsistency. The syntax of a logical language is designed to make these features formally explicit. As a result, determining properties like consistency can often be reduced to symbolic manipulation, that is, to a task that can be carried out by a computer. In order to pursue this approach, we first want to develop a technique for representing a possible situation. We do this in terms of something that logicians call a model.\n",
    "\n",
    "A **model** for a set W of sentences is a formal representation of a situation in which all the sentences in W are true. The usual way of representing models involves set theory. The domain D of discourse (all the entities we currently care about) is a set of individuals, while relations are treated as sets built up from D. Let's look at a concrete example. Our domain D will consist of three children, Stefan, Klaus and Evi, represented respectively as s, k and e. We write this as D = {s, k, e}. The expression boy denotes the set consisting of Stefan and Klaus, the expression girl denotes the set consisting of Evi, and the expression is running denotes the set consisting of Stefan and Evi. 1.2 is a graphical rendering of the model.\n",
    "\n",
    "Figure 1.2\n",
    "<img src=http://www.nltk.org/images/model_kids.png width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propositional Logic\n",
    "A logical language is designed to make reasoning formally explicit. As a result, it can capture aspects of natural language which determine whether a set of sentences is consistent. As part of this approach, we need to develop logical representations of a sentence φ which formally capture the **truth-conditions** of φ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propositional logic** allows us to represent just those parts of linguistic structure which correspond to certain sentential connectives. We have just looked at and. Other such connectives are not, or and if..., then.... In the formalization of propositional logic, the counterparts of such connectives are sometimes called **boolean operators**. The basic expressions of propositional logic are **propositional symbols**, often written as P, Q, R, etc. There are varying conventions for representing boolean operators. Since we will be focusing on ways of exploring logic within NLTK, we will stick to the following ASCII versions of the operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation       \t-\n",
      "conjunction    \t&\n",
      "disjunction    \t|\n",
      "implication    \t->\n",
      "equivalence    \t<->\n"
     ]
    }
   ],
   "source": [
    "nltk.boolean_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the propositional symbols and the boolean operators we can build an infinite set of **well formed formulas** (or just formulas, for short) of propositional logic. First, every propositional letter is a formula. Then if φ is a formula, so is -φ. And if φ and ψ are formulas, then so are (φ & ψ) (φ | ψ) (φ -> ψ) (φ <-> ψ).\n",
    "\n",
    "Table 2.1:\n",
    "\n",
    "Truth conditions for the Boolean Operators in Propositional Logic.\n",
    "\n",
    "|Boolean Operator|Truth Conditions|\n",
    "|----------------|----------------|\n",
    "|negation (it is not the case that ...)\t-φ is true in s|iff\tφ is false in s|\n",
    "|conjunction (and)\t(φ & ψ) is true in s|iff\tφ is true in s and ψ is true in s|\n",
    "|disjunction (or)\t(φ &#124; ψ) is true in s|iff\tφ is true in s or ψ is true in s|\n",
    "|implication (if ..., then ...)\t(φ -> ψ) is true in s|iff\tφ is false in s or ψ is true in s|\n",
    "|equivalence (if and only if)\t(φ <-> ψ) is true in s|iff\tφ and ψ are both true in s or both false in s|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NegatedExpression -(P & Q)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "read_expr('-(P & Q)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AndExpression (P & Q)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P & Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrExpression (P | (R -> Q))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P | (R -> Q)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IffExpression (P <-> --P)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P <-> -- P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical proofs can be carried out with NLTK's inference module, for example via an interface to the third-party theorem prover Prover9. The inputs to the inference mechanism first have to be converted into logical expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = nltk.sem.Expression.fromstring\n",
    "SnF = read_expr('SnF')\n",
    "NotFnS = read_expr('-FnS')\n",
    "R = read_expr('SnF -> -FnS')\n",
    "prover = nltk.Prover9()\n",
    "prover.prove(NotFnS,[SnF,R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = nltk.Valuation([('P',True),('Q',True),('R',False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dom = set()\n",
    "g = nltk.Assignment(dom)\n",
    "\n",
    "m = nltk.Model(dom,val)\n",
    "print(m.evaluate('(P & Q)',g))\n",
    "print(m.evaluate('-(P & Q)',g))\n",
    "print(m.evaluate('(P & R)',g))\n",
    "print(m.evaluate('(P | R)',g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Prder Logic\n",
    "## Syntax\n",
    "First-order logic keeps all the boolean operators of Propositional Logic. But it adds some important new mechanisms. To start with, propositions are analyzed into predicates and arguments, which takes us a step closer to the structure of natural languages. The standard construction rules for first-order logic recognize terms such as individual variables and individual constants, and predicates which take differing numbers of arguments. For example, *Angus walks* might be formalized as *walk(angus)* and *Angus sees Bertie* as *see(angus, bertie)*. We will call *walk* a **unary predicate**, and *see* a **binary predicate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to inspect the syntactic structure of expressions of first-order logic, and the usual way of doing this is to assign types to expressions. Following the tradition of Montague grammar, we will use two **basic types**: e is the type of entities, while t is the type of formulas, i.e., expressions which have truth values. Given these two basic types, we can form **complex types** for function expressions. That is, given any types σ and τ, 〈σ, τ〉 is a complex type corresponding to functions from 'σ things' to 'τ things'. For example, 〈e, t〉 is the type of expressions from entities to truth values, namely unary predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angus\n",
      "e\n",
      "walk\n",
      "<e,?>\n"
     ]
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "expr = read_expr('walk(angus)',type_check=True)\n",
    "print(expr.argument)\n",
    "print(expr.argument.type)\n",
    "print(expr.function)\n",
    "print(expr.function.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = {'walk':'<e,t>'}\n",
    "expr = read_expr('walk(angus)',signature=sig)\n",
    "expr.function.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance expr of this class comes with a method `free()` which returns the set of variables that are free in expr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{Variable('x')}\n",
      "set()\n",
      "set()\n",
      "{Variable('x')}\n",
      "{Variable('y')}\n"
     ]
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "print(read_expr('dog(cyril)').free())\n",
    "print(read_expr('dog(x)').free())\n",
    "print(read_expr('own(angus,cyril)').free())\n",
    "print(read_expr('exists x. dog(x)').free())\n",
    "print(read_expr('((some x. walk(x)) -> sing(x))').free())\n",
    "print(read_expr('exists x. own(y,x)').free())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Theorem Proving\n",
    "The general case in theorem proving is to determine whether a formula that we want to prove (a proof goal) can be derived by a finite sequence of inference steps from a list of assumed formulas. We write this as S ⊢ g, where S is a (possibly empty) list of assumptions, and g is a proof goal. We will illustrate this with NLTK's interface to the theorem prover Prover9. First, we parse the required proof goal and the two assumptions. Then we create a Prover9 instance, and call its prove() method on the goal, given the list of assumptions.\n",
    "```\n",
    "Sylvania is to the north of Freedonia. Therefore, Freedonia is not to the north of Sylvania\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NotFnS = read_expr('-north_of(f,s)')\n",
    "SnF = read_expr('north_of(s,f)')\n",
    "R = read_expr('all x. all y. (north_of(x,y) -> -north_of(y,x))')\n",
    "prover = nltk.Prover9()\n",
    "prover.prove(NotFnS,[SnF,R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FnS = read_expr('north_of(f,s)')\n",
    "prover.prove(FnS,[SnF,R])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the Language of First Order Logic\n",
    "We'll take this opportunity to restate our earlier syntactic rules for propositional logic and add the formation rules for quantifiers; together, these give us the syntax of first order logic. In addition, we make explicit the types of the expressions involved. We'll adopt the convention that 〈en, t〉 is the type of a predicate which combines with n arguments of type e to yield an expression of type t. In this case, we say that n is the arity of the predicate.\n",
    "\n",
    "1. If P is a predicate of type 〈en, t〉, and α1, ... αn are terms of type e, then P(α1, ... αn) is of type t.\n",
    "2. If α and β are both of type e, then (α = β) and (α != β) are of type t.\n",
    "3. If φ is of type t, then so is -φ.\n",
    "4. If φ and ψ are of type t, then so are (φ & ψ), (φ | ψ), (φ -> ψ) and (φ <-> ψ).\n",
    "5. If φ is of type t, and x is a variable of type e, then exists x.φ and  all x.φ are of type t.\n",
    "\n",
    "\n",
    "Table 3.1:\n",
    "\n",
    "Summary of new logical relations and operators required for First Order Logic, together with two useful methods of the Expression class.\n",
    "\n",
    "|Example|Description|\n",
    "|-------|-----------|\n",
    "|=|equality|\n",
    "|!=|inequality|\n",
    "|exists|existential quantifier|\n",
    "|all|universal quantifier|\n",
    "|e.free()|show free variables of e|\n",
    "|e.simplify()|carry out β-reduction on e|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth in Model\n",
    "Given a first-order logic language L, a model M for L is a pair 〈D, Val〉, where D is an nonempty set called the domain of the model, and Val is a function called the valuation function which assigns values from D to expressions of L as follows:\n",
    "\n",
    "- For every individual constant c in L, Val(c) is an element of D.\n",
    "- For every predicate symbol P of arity n ≥ 0, Val(P) is a function from Dn to {True, False}. (If the arity of P is 0, then Val(P) is simply a truth value, the P is regarded as a propositional symbol.)\n",
    "\n",
    "According to (ii), if P is of arity 2, then Val(P) will be a function f from pairs of elements of D to {True, False}. In the models we shall build in NLTK, we'll adopt a more convenient alternative, in which Val(P) is a set S of pairs, defined as follows:\n",
    "```\n",
    "(23)\t\tS = {s | f(s) = True}\n",
    "```\n",
    "Such an f is called the characteristic function of S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = {'b','o','c'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the utility function Valuation.fromstring() to convert a list of strings of the form symbol => value into a Valuation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bertie': 'b',\n",
      " 'boy': {('b',)},\n",
      " 'cyril': 'c',\n",
      " 'dog': {('c',)},\n",
      " 'girl': {('o',)},\n",
      " 'olive': 'o',\n",
      " 'see': {('o', 'c'), ('b', 'o'), ('c', 'b')},\n",
      " 'walk': {('c',), ('o',)}}\n"
     ]
    }
   ],
   "source": [
    "v = \"\"\"\n",
    "bertie => b\n",
    "olive => o\n",
    "cyril => c\n",
    "boy => {b}\n",
    "girl => {o}\n",
    "dog => {c}\n",
    "walk => {o, c}\n",
    "see => {(b, o), (c, b), (o, c)}\n",
    "\"\"\"\n",
    "val = nltk.Valuation.fromstring(v)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Variables and Assignments\n",
    "In our models, the counterpart of a context of use is a variable assignment. This is a mapping from individual variables to entities in the domain. Assignments are created using the Assignment constructor, which also takes the model's domain of discourse as a parameter. We are not required to actually enter any bindings, but if we do, they are in a (variable, value) format similar to what we saw earlier for valuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'o', 'y': 'c'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nltk.Assignment(dom, [('x', 'o'), ('y', 'c')])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g[c/y][o/x]\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nltk.Model(dom,val)\n",
    "m.evaluate('see(olive,y)',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate('see(y, x)', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method purge() clears all bindings from an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.purge()\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Undefined'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate('see(olive, y)', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate('see(bertie, olive) & boy(bertie) & -walk(bertie)', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantification\n",
    "One of the crucial insights of modern logic is that the notion of variable satisfaction can be used to provide an interpretation to quantified formulas. Let's use (24) as an example.\n",
    "```\n",
    "(24)\t\texists x.(girl(x) & walk(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate('exists x. (girl(x) & walk(x))',g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful tool offered by NLTK is the satisfiers() method. This returns a set of all the individuals that satisfy an open formula. The method parameters are a parsed formula, a variable, and an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'o'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmla1 = read_expr('girl(x) | boy(x)')\n",
    "m.satisfiers(fmla1,'x',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'c', 'o'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmla2 = read_expr('girl(x) -> walk(x)')\n",
    "m.satisfiers(fmla2,'x',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'o'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmla3 = read_expr('walk(x) -> girl(x)')\n",
    "m.satisfiers(fmla3,'x',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate('all x.(girl(x) -> walk(x))', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifier Scope Ambiguity\n",
    "What happens when we want to give a formal representation of a sentence with two quantifiers, such as the following?\n",
    "```\n",
    "(26)\t\tEverybody admires someone.\n",
    "```\n",
    "There are (at least) two ways of expressing (26) in first-order logic:\n",
    "```\n",
    "(27)\t\t\n",
    "a.\t\tall x.(person(x) -> exists y.(person(y) & admire(x,y)))\n",
    "\n",
    "b.\t\texists y.(person(y) & all x.(person(x) -> admire(x,y)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'e', 'j', 'm'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = \"\"\"\n",
    "bruce => b\n",
    "elspeth => e\n",
    "julia => j\n",
    "matthew => m\n",
    "person => {b, e, j, m}\n",
    "admire => {(j, b), (b, b), (m, e), (e, m)}\n",
    "\"\"\"\n",
    "val2 = nltk.Valuation.fromstring(v2)\n",
    "\n",
    "dom2 = val2.domain\n",
    "m2 = nltk.Model(dom2,val2)\n",
    "g2 = nltk.Assignment(dom2)\n",
    "fmla4 = read_expr('(person(x) -> exists y.(person(y) & admire(x, y)))')\n",
    "m2.satisfiers(fmla4, 'x', g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmla5 = read_expr('(person(y) & all x.(person(x) -> admire(x, y)))')\n",
    "m2.satisfiers(fmla5, 'y', g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmla6 = read_expr('(person(y) & all x.((x = bruce | x = julia) -> admire(x, y)))')\n",
    "m2.satisfiers(fmla6, 'y', g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "We invoke the Mace4 model builder by creating an instance of Mace() and calling its build_model() method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a3 = read_expr('exists x. (man(x) & walks(x))')\n",
    "c1 = read_expr('mortal(socrates)')\n",
    "c2 = read_expr('-mortal(socrates)')\n",
    "mb = nltk.Mace(5)\n",
    "print(mb.build_model(None,[a3,c1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(mb.build_model(None, [a3, c2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(mb.build_model(None, [c1, c2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the model builder as an adjunct to the theorem prover. Let's suppose we are trying to prove S ⊢ g, i.e. that g is logically derivable from assumptions S = [s1, s2, ..., sn]. We can feed this same input to Mace4, and the model builder will try to find a counterexample, that is, to show that g does not follow from S. So, given this input, Mace4 will try to find a model for the set S together with the negation of g, namely the list S' =\n",
    "[s1, s2, ..., sn, -g]. If g fails to follow from S, then Mace4 may well return with a counterexample faster than Prover9 concludes that it cannot find the required proof. Conversely, if g is provable from S, Mace4 may take a long time unsuccessfully trying to find a countermodel, and will eventually give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4 = read_expr('exists y. (woman(y) & all x. (man(x) -> love(x,y)))')\n",
    "a5 = read_expr('man(adam)')\n",
    "a6 = read_expr('woman(eve)')\n",
    "g = read_expr('love(adam,eve)')\n",
    "mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6])\n",
    "mc.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': 'b',\n",
      " 'adam': 'a',\n",
      " 'eve': 'a',\n",
      " 'love': {('a', 'b')},\n",
      " 'man': {('a',)},\n",
      " 'woman': {('a',), ('b',)}}\n"
     ]
    }
   ],
   "source": [
    "print(mc.valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of this valuation should be familiar to you: it contains some individual constants and predicates, each with an appropriate kind of value. What might be puzzling is the C1. This is a \"skolem constant\" that the model builder introduces as a representative of the existential quantifier. That is, when the model builder encountered the exists y part of a4 above, it knew that there is some individual b in the domain which satisfies the open formula in the body of a4. However, it doesn't know whether b is also the denotation of an individual constant anywhere else in its input, so it makes up a new name for b on the fly, namely  C1. Now, since our premises said nothing about the individual constants adam and eve, the model builder has decided there is no reason to treat them as denoting different entities, and they both get mapped to a. Moreover, we didn't specify that man and woman denote disjoint sets, so the model builder lets their denotations overlap. This illustrates quite dramatically the implicit knowledge that we bring to bear in interpreting our scenario, but which the model builder knows nothing about. So let's add a new assumption which makes the sets of men and women disjoint. The model builder still produces a countermodel, but this time it is more in accord with our intuitions about the situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a7 = read_expr('all x. (man(x) -> -woman(x))')\n",
    "g = read_expr('love(adam,eve)')\n",
    "mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6, a7])\n",
    "mc.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C1': 'c',\n",
      " 'adam': 'a',\n",
      " 'eve': 'b',\n",
      " 'love': {('a', 'c')},\n",
      " 'man': {('a',)},\n",
      " 'woman': {('c',), ('b',)}}\n"
     ]
    }
   ],
   "source": [
    "print(mc.valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics of English Sentences\n",
    "## Compositional Semantics in Feature-Based Grammar\n",
    "**Principle of Compositionality**: The meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined.\n",
    "\n",
    "\n",
    " (29) illustrates a first approximation to the kind of analyses we would like to build.\n",
    "\n",
    "(29)\t\t![](http://www.nltk.org/book/tree_images/ch10-tree-1.png)\n",
    "\n",
    "\n",
    "To complete the grammar is very straightforward; all we require are the rules shown below.\n",
    "```\n",
    "VP[SEM=?v] -> IV[SEM=?v]\n",
    "NP[SEM=<cyril>] -> 'Cyril'\n",
    "IV[SEM=<\\x.bark(x)>] -> 'barks'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The λ-Calculus\n",
    "We illustrated this with (31), which we glossed as \"the set of all w such that w is an element of V (the vocabulary) and w has property P\".\n",
    "```\n",
    "(31)\t\t{w | w ∈ V & P(w)}\n",
    "```\n",
    "It turns out to be extremely useful to add something to first-order logic that will achieve the same effect. We do this with the λ operator (pronounced \"lambda\"). The λ counterpart to (31) is (32). (Since we are not trying to do set theory here, we just treat V as a unary predicate.)\n",
    "```\n",
    "(32)\t\tλw. (V(w) ∧ P(w))\n",
    "```\n",
    "\n",
    "λ is a binding operator, just as the first-order logic quantifiers are. If we have an open formula such as (33a), then we can bind the variable x with the λ operator, as shown in (33b). The corresponding NLTK representation is given in (33c).\n",
    "```\n",
    "(33)\t\t\n",
    "a.\t\t(walk(x) ∧ chew_gum(x))\n",
    "\n",
    "b.\t\tλx.(walk(x) ∧ chew_gum(x))\n",
    "\n",
    "c.\t\t\\x.(walk(x) & chew_gum(x))\n",
    "```\n",
    "Remember that \\ is a special character in Python strings. We could escape it (with another \\), or else use \"raw strings\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LambdaExpression \\x.(walk(x) & chew_gum(x))>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "expr = read_expr(r'\\x.(walk(x) & chew_gum(x))')\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\x.(walk(x) & chew_gum(y))\n"
     ]
    }
   ],
   "source": [
    "print(read_expr(r'\\x.(walk(x) & chew_gum(y))'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a special name for the result of binding the variables in an expression: **λ abstraction**. When you first encounter λ-abstracts, it can be hard to get an intuitive sense of their meaning. A couple of English glosses for (33b) are: \"be an x such that x walks and x chews gum\" or \"have the property of walking and chewing gum\". It has often been suggested that λ-abstracts are good representations for verb phrases (or subjectless clauses), particularly when these occur as arguments in their own right. This is illustrated in (34a) and its translation (34b).\n",
    "```\n",
    "(34)\t\t\n",
    "a.\t\tTo walk and chew-gum is hard\n",
    "\n",
    "b.\t\thard(\\x.(walk(x) & chew_gum(x)))\n",
    "```\n",
    "\n",
    "So the general picture is this: given an open formula φ with free variable x, abstracting over x yields a property expression λx.φ — the property of being an x such that φ. Here's a more official version of how abstracts are built:\n",
    "```\n",
    "(35)\t\tIf α is of type τ, and x is a variable of type e, then \\x.α is of type 〈e, τ〉.\n",
    "```\n",
    "\n",
    "(34b) illustrated a case where we say something about a property, namely that it is hard. But what we usually do with properties is attribute them to individuals. And in fact if φ is an open formula, then the abstract λx.φ can be used as a unary predicate. In (36), (33b) is predicated of the term gerald.\n",
    "```\n",
    "(36)\t\t\\x.(walk(x) & chew_gum(x)) (gerald)\n",
    "```\n",
    "Now (36) says that Gerald has the property of walking and chewing gum, which has the same meaning as (37).\n",
    "```\n",
    "(37)\t\t(walk(gerald) & chew_gum(gerald))\n",
    "```\n",
    "\n",
    "We'll use α[β/x] as notation for the operation of replacing all free occurrences of x in α by the expression β. So:\n",
    "```\n",
    "(walk(x) & chew_gum(x))[gerald/x]\n",
    "```\n",
    "is the same expression as (37). The \"reduction\" of (36) to (37) is an extremely useful operation in simplifying semantic representations, and we shall use it a lot in the rest of this chapter. The operation is often called β-reduction. In order for it to be semantically justified, we want it to hold that λx. α(β) has the same semantic values as α[β/x].\n",
    "\n",
    " In order to carry of β-reduction of expressions in NLTK, we can call the simplify() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\x.(walk(x) & chew_gum(x))(gerald)\n"
     ]
    }
   ],
   "source": [
    "expr = read_expr(r'\\x.(walk(x) & chew_gum(x))(gerald)')\n",
    "print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(walk(gerald) & chew_gum(gerald))\n"
     ]
    }
   ],
   "source": [
    "print(expr.simplify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\y.(dog(cyril) & own(y,cyril))\n",
      "(dog(cyril) & own(angus,cyril))\n"
     ]
    }
   ],
   "source": [
    "print(read_expr(r'\\x.\\y.(dog(x) & own(y, x))(cyril)').simplify())\n",
    "print(read_expr(r'\\x y.(dog(x) & own(y, x))(cyril, angus)').simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our λ abstracts so far have involved the familiar first order variables: x, y and so on — variables of type e. But suppose we want to treat one abstract, say \\x.walk(x) as the argument of another λ abstract? We might try this:\n",
    "```\n",
    "\\y.y(angus)(\\x.walk(x))\n",
    "```\n",
    "But since the variable y is stipulated to be of type e, \\y.y(angus) only applies to arguments of type e while \\x.walk(x) is of type 〈e, t〉! Instead, we need to allow abstraction over variables of higher type. Let's use P and Q as variables of type 〈e, t〉, and then we can have an abstract such as \\P.P(angus). Since P is of type 〈e, t〉, the whole abstract is of type 〈〈e, t〉, t〉. Then \\P.P(angus)(\\x.walk(x)) is legal, and can be simplified via β-reduction to  \\x.walk(x)(angus) and then again to walk(angus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When carrying out β-reduction, some care has to be taken with variables. Consider, for example, the λ terms (39a) and (39b), which differ only in the identity of a free variable.\n",
    "```\n",
    "(39)\t\t\n",
    "a.\t\t\\y.see(y, x)\n",
    "\n",
    "b.\t\t\\y.see(y, z)\n",
    "```\n",
    "\n",
    "Suppose now that we apply the λ-term \\P.exists x.P(x) to each of these terms:\n",
    "```\n",
    "(40)\t\t\n",
    "a.\t\t\\P.exists x.P(x)(\\y.see(y, x))\n",
    "\n",
    "b.\t\t\\P.exists x.P(x)(\\y.see(y, z))\n",
    "```\n",
    "\n",
    "We pointed out earlier that the results of the application should be semantically equivalent. But if we let the free variable x in (39a) fall inside the scope of the existential quantifier in (40a), then after reduction, the results will be different:\n",
    "```\n",
    "(41)\t\t\n",
    "a.\t\texists x.see(x, x)\n",
    "\n",
    "b.\t\texists x.see(x, z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any variable-binding expression (involving ∀, ∃ or λ), the name chosen for the bound variable is completely arbitrary. For example, exists x.P(x) and exists y.P(y) are equivalent; they are called **α equivalents**, or **alphabetic variants**. The process of relabeling bound variables is known as **α-conversion**. When we test for equality of VariableBinderExpressions in the logic module (i.e., using ==), we are in fact testing for α-equivalence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists x.P(x)\n"
     ]
    }
   ],
   "source": [
    "expr1 = read_expr('exists x.P(x)')\n",
    "print(expr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists z.P(z)\n"
     ]
    }
   ],
   "source": [
    "expr2 = expr1.alpha_convert(nltk.sem.Variable('z'))\n",
    "print(expr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr1 == expr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When β-reduction is carried out on an application f(a), we check whether there are free variables in a which also occur as bound variables in any subterms of f. Suppose, as in the example discussed above, that x is free in a, and that f contains the subterm exists x.P(x). In this case, we produce an alphabetic variant of exists x.P(x), say, exists z1.P(z1), and then carry on with the reduction. This relabeling is carried out automatically by the β-reduction code in  logic, and the results can be seen in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\\P.exists x.P(x))(\\y.see(y,x))\n"
     ]
    }
   ],
   "source": [
    "expr3 = read_expr('\\P.(exists x.P(x))(\\y.see(y, x))')\n",
    "print(expr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists z1.see(z1,x)\n"
     ]
    }
   ],
   "source": [
    "print(expr3.simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantified NPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we want (42a) to be given the logical form in (42b). How can this be accomplished?\n",
    "```\n",
    "(42)\t\t\n",
    "a.\t\tA dog barks.\n",
    "\n",
    "b.\t\texists x.(dog(x) & bark(x))\n",
    "```\n",
    "\n",
    "As a first step, let's make the subject's sem value act as the function expression rather than the argument. (This is sometimes called type-raising.) Now we are looking for way of instantiating  ?np so that [SEM=<?np(\\x.bark(x))>] is equivalent to [SEM=<exists x.(dog(x) & bark(x))>]. We replace the occurrence of 'bark' in (42b) by a predicate variable 'P', and bind the variable with λ, as shown in (43).\n",
    "```\n",
    "(43)\t\t\\P.exists x.(dog(x) & P(x))\n",
    "```\n",
    "\n",
    "We have used a different style of variable in (43) — that is 'P' rather than 'x' or 'y' — to signal that we are abstracting over a different kind of object — not an individual, but a function expression of type 〈e, t〉. So the type of (43) as a whole is 〈〈e, t〉, t〉. We will take this to be the type of NPs in general. To illustrate further, a universally quantified NP will look like (44).\n",
    "```\n",
    "(44)\t\t\\P.all x.(dog(x) -> P(x))\n",
    "```\n",
    "\n",
    "We are pretty much done now, except that we also want to carry out a further abstraction plus application for the process of combining the semantics of the determiner a, namely (43), with the semantics of dog.\n",
    "```\n",
    "(45)\t\t\\Q P.exists x.(Q(x) & P(x))\n",
    "```\n",
    "\n",
    "Applying (45) as a function expression to dog yields (43), and applying that to bark gives us \\P.exists x.(dog(x) & P(x))(\\x.bark(x)). Finally, carrying out β-reduction yields just what we wanted, namely (42b).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitive Verbs\n",
    "Our next challenge is to deal with sentences containing transitive verbs, such as (46).\n",
    "```\n",
    "(46)\t\tAngus chases a dog.\n",
    "(47)\t\t\\y.exists x.(dog(x) & chase(y, x))\n",
    "```\n",
    "\n",
    "ur task now resolves to designing a semantic representation for chases which can combine with (43) so as to allow (47) to be derived.\n",
    "\n",
    "Let's carry out the inverse of β-reduction on (47), giving rise to (48).\n",
    "```\n",
    "(48)\t\t\\P.exists x.(dog(x) & P(x))(\\z.chase(y, z))\n",
    "```\n",
    "\n",
    "(48) is equivalent via β-reduction to exists x.(dog(x) & chase(y, x)).\n",
    "\n",
    "Now let's replace the function expression in (48) by a variable X of the same type as an NP; that is, of type 〈〈e, t〉, t〉.\n",
    "```\n",
    "(49)\t\tX(\\z.chase(y, z))\n",
    "```\n",
    "\n",
    "The representation of a transitive verb will have to apply to an argument of the type of X to yield a function expression of the type of VPs, that is, of type 〈e, t〉. We can ensure this by abstracting over both the X variable in (49) and also the subject variable y. So the full solution is reached by giving chases the semantic representation shown in (50).\n",
    "```\n",
    "(50)\t\t\\X y.X(\\x.chase(y, x))\n",
    "```\n",
    "\n",
    "If (50) is applied to (43), the result after β-reduction is equivalent to (47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\\X x.X(\\x.chase(y,x)))(\\P.exists x.(dog(x) & P(x)))\n"
     ]
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "tvp = read_expr(r'\\X x.X(\\x.chase(y,x))')\n",
    "np = read_expr(r'(\\P.exists x.(dog(x) & P(x)))')\n",
    "vp = nltk.sem.ApplicationExpression(tvp,np)\n",
    "print(vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\x.exists x.(dog(x) & chase(y,x))\n"
     ]
    }
   ],
   "source": [
    "print(vp.simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(51)\t\t\\P.P(angus)\n",
    "```\n",
    "(51) denotes the characteristic function corresponding to the set of all properties which are true of Angus. Converting from an individual constant angus to \\P.P(angus) is another example of type-raising, briefly mentioned earlier, and allows us to replace a Boolean-valued application such as \\x.walk(x)(angus) with an equivalent function application \\P.P(angus)(\\x.walk(x)). By β-reduction, both expressions reduce to walk(angus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all z2.(dog(z2) -> exists z1.(bone(z1) & give(angus,z1,z2)))\n"
     ]
    }
   ],
   "source": [
    "from nltk import load_parser\n",
    "parser = load_parser('grammars/book_grammars/simple-sem.fcfg', trace=0)\n",
    "sentence = 'Angus gives a bone to every dog'\n",
    "tokens = sentence.split()\n",
    "for tree in parser.parse(tokens):\n",
    "    print(tree.label()['SEM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides some utilities to make it easier to derive and inspect semantic interpretations. The function interpret_sents() is intended for interpretation of a list of input sentences. It builds a dictionary d where for each sentence  sent in the input, d[sent] is a list of pairs (synrep, semrep) consisting of trees and semantic representations for sent. The value is a list since sent may be syntactically ambiguous; in the following example, however, there is only one parse tree per sentence in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[SEM=<walk(irene)>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(irene)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(irene)>] Irene))\n",
      "  (VP[NUM='sg', SEM=<\\x.walk(x)>]\n",
      "    (IV[NUM='sg', SEM=<\\x.walk(x)>, TNS='pres'] walks)))\n",
      "(S[SEM=<exists z13.(ankle(z13) & bite(cyril,z13))>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(cyril)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(cyril)>] Cyril))\n",
      "  (VP[NUM='sg', SEM=<\\x.exists z13.(ankle(z13) & bite(x,z13))>]\n",
      "    (TV[NUM='sg', SEM=<\\X x.X(\\y.bite(x,y))>, TNS='pres'] bites)\n",
      "    (NP[NUM='sg', SEM=<\\Q.exists x.(ankle(x) & Q(x))>]\n",
      "      (Det[NUM='sg', SEM=<\\P Q.exists x.(P(x) & Q(x))>] an)\n",
      "      (Nom[NUM='sg', SEM=<\\x.ankle(x)>]\n",
      "        (N[NUM='sg', SEM=<\\x.ankle(x)>] ankle)))))\n"
     ]
    }
   ],
   "source": [
    "sents = ['Irene walks', 'Cyril bites an ankle']\n",
    "grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
    "for results in nltk.interpret_sents(sents, grammar_file):\n",
    "    for (synrep, semrep) in results:\n",
    "        print(synrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen now how to convert English sentences into logical forms, and earlier we saw how logical forms could be checked as true or false in a model. Putting these two mappings together, we can check the truth value of English sentences in a given model. Let's take model m as defined above. The utility evaluate_sents() resembles interpret_sents() except that we need to pass a model and a variable assignment as parameters. The output is a triple (synrep, semrep, value) where synrep, semrep are as before, and value is a truth value. For simplicity, the following example only processes a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all z14.(boy(z14) -> see(cyril,z14))\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "v = \"\"\"\n",
    "bertie => b\n",
    "olive => o\n",
    "cyril => c\n",
    "boy => {b}\n",
    "girl => {o}\n",
    "dog => {c}\n",
    "walk => {o, c}\n",
    "see => {(b, o), (c, b), (o, c)}\n",
    "\"\"\"\n",
    "val = nltk.Valuation.fromstring(v)\n",
    "g = nltk.Assignment(val.domain)\n",
    "m = nltk.Model(val.domain, val)\n",
    "sent = 'Cyril sees every boy'\n",
    "grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
    "results = nltk.evaluate_sents([sent], grammar_file, m, g)[0]\n",
    "for (syntree, semrep, value) in results:\n",
    "    print(semrep)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifier Ambiguity Revisited\n",
    "One important limitation of the methods described above is that they do not deal with scope ambiguity. Our translation method is syntax-driven, in the sense that the semantic representation is closely coupled with the syntactic analysis, and the scope of the quantifiers in the semantics therefore reflects the relative scope of the corresponding NP s in the syntactic parse tree. Consequently, a sentence like (52), will always be translated as (53a), not (53b)\n",
    "```\n",
    "(52)\t\tEvery girl chases a dog.\n",
    "\n",
    "(53)\t\t\n",
    "a.\t\tall x.(girl(x) -> exists y.(dog(y) & chase(x,y)))\n",
    "\n",
    "b.\t\texists y.(dog(y) & all x.(girl(x) -> chase(x,y)))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/quant-ambig.png\" alt=\"quantifier_scopings\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the method known as **Cooper storage**, a semantic representation is no longer an expression of first-order logic, but instead a pair consisting of a \"core\" semantic representation plus a list of binding operators. For the moment, think of a binding operator as being identical to the semantic representation of a quantified NP such as (44) or (45). Following along the lines indicated in the above figure, let's assume that we have constructed a Cooper-storage style semantic representation of sentence (52), and let's take our core to be the open formula chase(x,y). Given a list of binding operators corresponding to the two NPs in (52), we pick a binding operator off the list, and combine it with the core.\n",
    "```\n",
    "\\P.exists y.(dog(y) & P(y))(\\z2.chase(z1,z2))\n",
    "```\n",
    "Then we take the result, and apply the next binding operator from the list to it.\n",
    "```\n",
    "\\P.all x.(girl(x) -> P(x))(\\z1.exists x.(dog(x) & chase(z1,x)))\n",
    "```\n",
    "Once the list is empty, we have a conventional logical form for the sentence. Combining binding operators with the core in this way is called **S-Retrieval**. If we are careful to allow every possible order of binding operators, then we will be able to generate every possible scope ordering of quantifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question to address is how we build up a core+store representation compositionally. As before, each phrasal and lexical rule in the grammar will have a sem feature, but now there will be embedded features core and store. To illustrate the machinery, let's consider a simpler example, namely Cyril smiles. Here's a lexical rule for the verb smiles (taken from the grammar storage.fcfg) which looks pretty innocuous.\n",
    "```\n",
    "IV[SEM=[core=<\\x.smile(x)>, store=(/)]] -> 'smiles'\n",
    "```\n",
    "\n",
    "The rule for the proper name Cyril is more complex.\n",
    "```\n",
    "NP[SEM=[core=<@x>, store=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril'\n",
    "```\n",
    "\n",
    "The bo predicate has two subparts: the standard (type-raised) representation of a proper name, and the expression @x, which is called the address of the binding operator. (We'll explain the need for the address variable shortly.) @x is a metavariable, that is, a variable that ranges over individual variables of the logic and, as you will see, it also provides the value of core. The rule for VP just percolates up the semantics of the  IV, and the interesting work is done by the  S rule.\n",
    "```\n",
    "VP[SEM=?s] -> IV[SEM=?s]\n",
    "\n",
    "S[SEM=[core=<?vp(?np)>, store=(?b1+?b2)]] ->\n",
    "   NP[SEM=[core=?np, store=?b1]] VP[SEM=[core=?vp, store=?b2]]\n",
    "```\n",
    "\n",
    "The core value at the S node is the result of applying the VP's core value, namely \\x.smile(x), to the subject NP's value. The latter will not be @x, but rather an instantiation of @x, say z3. After β-reduction, <?vp(?np)> will be unified with <smile(z3)>. Now, when @x is instantiated as part of the parsing process, it will be instantiated uniformly. In particular, the occurrence of @x in the subject NP's store will also be mapped to z3, yielding the element  bo(\\P.P(cyril),z3). These steps can be seen in the following parse tree.\n",
    "```\n",
    "(S[SEM=[core=<smile(z3)>, store=(bo(\\P.P(cyril),z3))]]\n",
    "  (NP[SEM=[core=<z3>, store=(bo(\\P.P(cyril),z3))]] Cyril)\n",
    "  (VP[SEM=[core=<\\x.smile(x)>, store=()]]\n",
    "    (IV[SEM=[core=<\\x.smile(x)>, store=()]] smiles)))\n",
    "```\n",
    "Let's return to our more complex example, (52), and see what the storage style sem value is, after parsing with grammar storage.fcfg.\n",
    "```\n",
    "core  = <chase(z1,z2)>\n",
    "store = (bo(\\P.all x.(girl(x) -> P(x)),z1), bo(\\P.exists x.(dog(x) & P(x)),z2))\n",
    "```\n",
    "It should be clearer now why the address variables are an important part of the binding operator. Recall that during S-retrieval, we will be taking binding operators off the store list and applying them successively to the core. Suppose we start with bo(\\P.all x.(girl(x)\n",
    "-> P(x)),z1), which we want to combine with chase(z1,z2). The quantifier part of binding operator is \\P.all x.(girl(x) -> P(x)), and to combine this with  chase(z1,z2), the latter needs to first be turned into a λ-abstract. How do we know which variable to abstract over? This is what the address z1 tells us; i.e. that every girl has the role of chaser rather than chasee.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `nltk.sem.cooper_storage` deals with the task of turning storage-style semantic representations into standard logical forms. First, we construct a CooperStore instance, and inspect its store and core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chase(z2,z3)\n"
     ]
    }
   ],
   "source": [
    "from nltk.sem import cooper_storage as cs\n",
    "sentence = 'every girl chases a dog'\n",
    "trees = cs.parse_with_bindops(sentence, grammar='grammars/book_grammars/storage.fcfg')\n",
    "semrep = trees[0].label()['SEM']\n",
    "cs_semrep = cs.CooperStore(semrep)\n",
    "print(cs_semrep.core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bo(\\P.all x.(girl(x) -> P(x)),z2)\n",
      "bo(\\P.exists x.(dog(x) & P(x)),z3)\n"
     ]
    }
   ],
   "source": [
    "for bo in cs_semrep.store:\n",
    "    print(bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation 1\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z2.chase(z2,z3))\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z3.all x.(girl(x) -> chase(x,z3)))\n",
      "Permutation 2\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z3.chase(z2,z3))\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z2.exists x.(dog(x) & chase(z2,x)))\n"
     ]
    }
   ],
   "source": [
    "cs_semrep.s_retrieve(trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists x.(dog(x) & all z17.(girl(z17) -> chase(z17,x)))\n",
      "all x.(girl(x) -> exists z18.(dog(z18) & chase(x,z18)))\n"
     ]
    }
   ],
   "source": [
    "for reading in cs_semrep.readings:\n",
    "    print(reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descourse Semantics\n",
    "A discourse is a sequence of sentences. Very often, the interpretation of a sentence in a discourse depends what preceded it.\n",
    "\n",
    "## Desicourse Representation Theory\n",
    "The standard approach to quantification in first-order logic is limited to single sentences. Yet there seem to be examples where the scope of a quantifier can extend over two or more sentences. We saw one above, and here's a second example, together with a translation.\n",
    "```\n",
    "(54)\t\t\n",
    "a.\t\tAngus owns a dog. It bit Irene.\n",
    "\n",
    "b.\t\t∃x.(dog(x) ∧ own(Angus, x) ∧ bite(x, Irene))\n",
    "```\n",
    "\n",
    "Discourse Representation Theory (DRT) was developed with the specific goal of providing a means for handling this and other semantic phenomena which seem to be characteristic of discourse. A **discourse representation structure** (DRS) presents the meaning of discourse in terms of a list of discourse referents and a list of conditions. The discourse referents are the things under discussion in the discourse, and they correspond to the individual variables of first-order logic. The **DRS conditions** apply to those discourse referents, and correspond to atomic open formulas of first-order logic. 5.1 illustrates how DRS for the first sentence in (54a) is augmented to become a DRS for both sentences.\n",
    "\n",
    "Figure 5.1: Building a DRS\n",
    "<img src=http://www.nltk.org/images/drs1.png width=\"500x\">\n",
    "\n",
    "In order to process DRSs computationally, we need to convert them into a linear format. Here's an example, where the DRS is a pair consisting of a list of discourse of referents and a list of DRS conditions:\n",
    "```\n",
    "([x, y], [angus(x), dog(y), own(x,y)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x,y],[angus(x), dog(y), own(x,y)])\n"
     ]
    }
   ],
   "source": [
    "read_dexpr = nltk.sem.DrtExpression.fromstring\n",
    "drs1 = read_dexpr('([x, y], [angus(x), dog(y), own(x, y)])')\n",
    "print(drs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs1.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists x y.(angus(x) & dog(y) & own(x,y))\n"
     ]
    }
   ],
   "source": [
    "print(drs1.fol())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRT Expressions have a DRS-concatenation operator, represented as the + symbol. The concatenation of two DRSs is a single DRS containing the merged discourse referents and the conditions from both arguments. DRS-concatenation automatically α-converts bound variables to avoid name-clashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([x],[walk(x)]) + ([y],[run(y)]))\n"
     ]
    }
   ],
   "source": [
    "drs2 = read_dexpr('([x],[walk(x)]) + ([y],[run(y)])')\n",
    "print(drs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x,y],[walk(x), run(y)])\n"
     ]
    }
   ],
   "source": [
    "print(drs2.simplify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all x.(dog(x) -> exists y.(ankle(y) & bite(x,y)))\n"
     ]
    }
   ],
   "source": [
    "drs3 = read_dexpr('([], [(([x], [dog(x)]) -> ([y],[ankle(y), bite(x, y)]))])')\n",
    "print(drs3.fol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u,x,y,z],[angus(x), dog(y), own(x,y), PRO(u), irene(z), bite(u,z)])\n"
     ]
    }
   ],
   "source": [
    "drs4 = read_dexpr('([x, y], [angus(x), dog(y), own(x, y)])')\n",
    "drs5 = read_dexpr('([u, z], [PRO(u), irene(z), bite(u, z)])')\n",
    "drs6 = drs4 + drs5\n",
    "print(drs6.simplify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u,x,y,z],[angus(x), dog(y), own(x,y), (u = [x,y,z]), irene(z), bite(u,z)])\n"
     ]
    }
   ],
   "source": [
    "print(drs6.simplify().resolve_anaphora())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our treatment of DRSs is fully compatible with the existing machinery for handling λ abstraction, and consequently it is straightforward to build compositional semantic representations which are based on DRT rather than first-order logic. This technique is illustrated in the following rule for indefinites (which is part of the grammar drt.fcfg). For ease of comparison, we have added the parallel rule for indefinites from simple-sem.fcfg.\n",
    "```\n",
    "Det[num=sg,SEM=<\\P Q.(([x],[]) + P(x) + Q(x))>] -> 'a'\n",
    "Det[num=sg,SEM=<\\P Q. exists x.(P(x) & Q(x))>] -> 'a'\n",
    "```\n",
    "To get a better idea of how the DRT rule works, look at this subtree for the NP a dog.\n",
    "```\n",
    "(NP[num='sg', SEM=<\\Q.(([x],[dog(x)]) + Q(x))>]\n",
    "  (Det[num='sg', SEM=<\\P Q.((([x],[]) + P(x)) + Q(x))>] a)\n",
    "  (Nom[num='sg', SEM=<\\x.([],[dog(x)])>]\n",
    "    (N[num='sg', SEM=<\\x.([],[dog(x)])>] dog)))))\n",
    "```\n",
    "\n",
    "The λ abstract for the indefinite is applied as a function expression to \\x.([],[dog(x)]) which leads to \\Q.(([x],[]) + ([],[dog(x)]) +\n",
    "Q(x)); after simplification, we get \\Q.(([x],[dog(x)]) + Q(x)) as the representation for the NP as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x,z3],[Angus(x), dog(z3), own(x,z3)])\n"
     ]
    }
   ],
   "source": [
    "from nltk import load_parser\n",
    "parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.sem.drt.DrtParser())\n",
    "trees = list(parser.parse('Angus owns a dog'.split()))\n",
    "print(trees[0].label()['SEM'].simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse Processing\n",
    "Whereas a discourse is a sequence s1, ... sn of sentences, a discourse thread is a sequence s1-ri, ... sn-rj of readings, one for each sentence in the discourse. The module processes sentences incrementally, keeping track of all possible threads when there is ambiguity. For simplicity, the following example ignores scope ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "s0 readings:\n",
      "\n",
      "s0-r0: exists z1.(student(z1) & dance(z1))\n",
      "\n",
      "s1 readings:\n",
      "\n",
      "s1-r0: all z1.(student(z1) -> person(z1))\n"
     ]
    }
   ],
   "source": [
    "dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
    "dt.readings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a new sentence is added to the current discourse, setting the parameter consistchk=True causes consistency to be checked by invoking the model checker for each thread, i.e., sequence of admissible readings. In this case, the user has the option of retracting the sentence in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent discourse: d0 ['s0-r0', 's1-r0', 's2-r0']:\n",
      "    s0-r0: exists z1.(student(z1) & dance(z1))\n",
      "    s1-r0: all z1.(student(z1) -> person(z1))\n",
      "    s2-r0: -exists z1.(person(z1) & dance(z1))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.add_sentence('No person dances', consistchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sentences are \n",
      "s0: A student dances\n",
      "s1: Every student is a person\n"
     ]
    }
   ],
   "source": [
    "dt.retract_sentence('No person dances', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we use informchk=True to check whether a new sentence φ is informative relative to the current discourse. The theorem prover treats existing sentences in the thread as assumptions and attempts to prove φ; it is informative if no such proof can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'A person dances' under reading 'exists x.(person(x) & dance(x))':\n",
      "Not informative relative to thread 'd0'\n"
     ]
    }
   ],
   "source": [
    "dt.add_sentence('A person dances', informchk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- First order logic is a suitable language for representing natural language meaning in a computational setting since it is flexible enough to represent many useful aspects of natural meaning, and there are efficient theorem provers for reasoning with first order logic. (Equally, there are a variety of phenomena in natural language semantics which are believed to require more powerful logical mechanisms.)\n",
    "- As well as translating natural language sentences into first order logic, we can state the truth conditions of these sentences by examining models of first order formulas.\n",
    "- In order to build meaning representations compositionally, we supplement first order logic with the λ calculus.\n",
    "- β-reduction in the λ-calculus corresponds semantically to application of a function to an argument. Syntactically, it involves replacing a variable bound by λ in the function expression with the expression that provides the argument in the function application.\n",
    "- A key part of constructing a model lies in building a valuation which assigns interpretations to non-logical constants. These are interpreted as either n-ary predicates or as individual constants.\n",
    "- An open expression is an expression containing one or more free variables. Open expressions only receive an interpretation when their free variables receive values from a variable assignment.\n",
    "- Quantifiers are interpreted by constructing, for a formula φ[x] open in variable x, the set of individuals which make φ[x] true when an assignment g assigns them as the value of x. The quantifier then places constraints on that set.\n",
    "- A closed expression is one that has no free variables; that is, the variables are all bound. A closed sentence is true or false with respect to all variable assignments.\n",
    "- If two formulas differ only in the label of the variable bound by binding operator (i.e, λ or a quantifier) , they are said to be α equivalents. The result of relabeling a bound variable in a formula is called α-conversion.\n",
    "- Given a formula with two nested quantifiers Q1 and Q2, the outermost quantifier Q1 is said to have wide scope (or scope over Q2). English sentences are frequently ambiguous with respect to the scope of the quantifiers they contain.\n",
    "- English sentences can be associated with a semantic representation by treating sem as a feature in a feature-based grammar. The sem value of a complex expressions typically involves functional application of the sem values of the component expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
