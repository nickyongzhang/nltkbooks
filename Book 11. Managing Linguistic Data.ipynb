{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Structure: A Case Study\n",
    "The TIMIT corpus of read speech was the first annotated speech database to be widely distributed, and it has an especially clear organization. TIMIT was developed by a consortium including Texas Instruments and MIT, from which it derives its name. It was designed to provide data for the acquisition of acoustic-phonetic knowledge and to support the development and evaluation of automatic speech recognition systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Structure of TIMIT\n",
    "Figure 1.1: Structure of a TIMIT Identifier\n",
    "<img src='http://www.nltk.org/images/timit.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentences:\n",
    "```\n",
    "(1)\t\t\n",
    "a.\t\tshe had your dark suit in greasy wash water all year\n",
    "\n",
    "b.\t\tdon't ask me to carry an oily rag like that\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl', 's', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
    "print(phonetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 7812, 10610),\n",
       " ('had', 10610, 14496),\n",
       " ('your', 14496, 15791),\n",
       " ('dark', 15791, 20720),\n",
       " ('suit', 20720, 25647),\n",
       " ('in', 25647, 26906),\n",
       " ('greasy', 26906, 32668),\n",
       " ('wash', 32668, 37890),\n",
       " ('water', 38531, 42417),\n",
       " ('all', 43091, 46052),\n",
       " ('year', 46052, 50522)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.word_times('dr1-fvmh0/sa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timitdict = nltk.corpus.timit.transcription_dict()\n",
    "timitdict['greasy'] + timitdict['wash'] + timitdict['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic[17:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86', birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS', comments='BEST NEW ENGLAND ACCENT SO FAR')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.spkrinfo('dr1-fvmh0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notable Design Features\n",
    "TIMIT illustrates several key features of corpus design. First, the corpus contains two layers of annotation, at the phonetic and orthographic levels. In general, a text or speech corpus may be annotated at many different linguistic levels, including morphological, syntactic, and discourse levels. Moreover, even at a given level there may be different labeling schemes or even disagreement amongst annotators, such that we want to represent multiple versions. A second property of TIMIT is its balance across multiple dimensions of variation, for coverage of dialect regions and diphones. The inclusion of speaker demographics brings in many more independent variables, that may help to account for variation in the data, and which facilitate later uses of the corpus for purposes that were not envisaged when the corpus was created, such as sociolinguistics. A third property is that there is a sharp division between the original linguistic event captured as an audio recording, and the annotations of that event. The same holds true of text corpora, in the sense that the original text usually has an external source, and is considered to be an immutable artifact. Any transformations of that artifact which involve human judgment — even something as simple as tokenization — are subject to later revision, thus it is important to retain the source material in a form that is as close to the original as possible.\n",
    "A fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per sentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These are organized into a tree structure, shown schematically in 1.2. At the top level there is a split between training and testing sets, which gives away its intended use for developing and evaluating statistical models.\n",
    "\n",
    "Finally, notice that even though TIMIT is a speech corpus, its transcriptions and associated data are just text, and can be processed using programs just like any other text corpus. Therefore, many of the computational methods described in this book are applicable. Moreover, notice that all of the data types included in the TIMIT corpus fall into the two basic categories of lexicon and text, which we will discuss below. Even the speaker demographics data is just another instance of the lexicon data type.\n",
    "\n",
    "Figure 1.2: Structure of the Published TIMIT Corpus: The CD-ROM contains doc, train, and test directories at the top level; the train and test directories both have 8 sub-directories, one per dialect region; each of these contains further subdirectories, one per speaker; the contents of the directory for female speaker aks0 are listed, showing 10 wav files accompanied by a text transcription, a word-aligned transcription, and a phonetic transcription.\n",
    "<img src='http://www.nltk.org/images/timit-structure.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Data Types\n",
    "Figure 1.3: Basic Linguistic Data Types — Lexicons and Texts: amid their diversity, lexicons have a record structure, while annotated texts have a temporal organization.\n",
    "<img src='http://www.nltk.org/images/datatypes.png' width=500>\n",
    "    \n",
    "espite its complexity, the TIMIT corpus only contains two fundamental data types, namely lexicons and texts. As we saw in 2., most lexical resources can be represented using a record structure, i.e. a key plus one or more fields, as shown in 1.3. A lexical resource could be a conventional dictionary or comparative wordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a phrase rather than a single word. A thesaurus also consists of record-structured data, where we look up entries via non-key fields that correspond to topics. We can also construct special tabulations (known as paradigms) to illustrate contrasts and systematic variation, as shown in 1.3 for three verbs. TIMIT's speaker table is also a kind of lexicon.\n",
    "\n",
    "At the most abstract level, a text is a representation of a real or fictional speech event, and the time-course of that event carries over into the text itself. A text could be a small unit, such as a word or sentence, or a complete narrative or dialogue. It may come with annotations such as part-of-speech tags, morphological analysis, discourse structure, and so forth. As we saw in the IOB tagging technique (7.), it is possible to represent higher-level constituents using tags on individual words. Thus the abstraction of text shown in 1.3 is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Life-Cycle of a Corpus\n",
    "## Three Corpus Creattion Scenarios\n",
    "In one type of corpus, the design unfolds over in the course of the creator's explorations. This is the pattern typical of traditional \"field linguistics,\" in which material from elicitation sessions is analyzed as it is gathered, with tomorrow's elicitation often based on questions that arise in analyzing today's. The resulting corpus is then used during subsequent years of research, and may serve as an archival resource indefinitely. Computerization is an obvious boon to work of this type, as exemplified by the popular program Shoebox, now over two decades old and re-released as Toolbox (see 4). Other software tools, even simple word processors and spreadsheets, are routinely used to acquire the data. In the next section we will look at how to extract data from these sources.\n",
    "\n",
    "Another corpus creation scenario is typical of experimental research where a body of carefully-designed material is collected from a range of human subjects, then analyzed to evaluate a hypothesis or develop a technology. It has become common for such databases to be shared and re-used within a laboratory or company, and often to be published more widely. Corpora of this type are the basis of the \"common task\" method of research management, which over the past two decades has become the norm in government-funded research programs in language technology. We have already encountered many such corpora in the earlier chapters; we will see how to write Python programs to implement the kinds of curation tasks that are necessary before such corpora are published.\n",
    "\n",
    "Finally, there are efforts to gather a \"reference corpus\" for a particular language, such as the American National Corpus (ANC) and the British National Corpus (BNC). Here the goal has been to produce a comprehensive record of the many forms, styles and uses of a language. Apart from the sheer challenge of scale, there is a heavy reliance on automatic annotation tools together with post-editing to fix any errors. However, we can write programs to locate and repair the errors, and also to analyze the corpus for balance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control\n",
    "Large annotation tasks require multiple annotators, which raises the problem of achieving consistency. How consistently can a group of annotators perform? We can easily measure consistency by having a portion of the source material independently annotated by two people. This may reveal shortcomings in the guidelines or differing abilities with the annotation task. In cases where quality is paramount, the entire corpus can be annotated twice, and any inconsistencies adjudicated by an expert.  \n",
    "It is considered best practice to report the inter-annotator agreement that was achieved for a corpus (e.g. by double-annotating 10% of the corpus). This score serves as a helpful upper bound on the expected performance of any automatic system that is trained on this corpus.\n",
    "\n",
    "The **Kappa** coefficient K measures agreement between two people making category judgments, correcting for expected chance agreement. For example, suppose an item is to be annotated, and four coding options are equally likely. Then two people coding randomly would be expected to agree 25% of the time. Thus, an agreement of 25% will be assigned K = 0, and better levels of agreement will be scaled accordingly. For an agreement of 50%, we would get K = 0.333, as 50 is a third of the way from 25 to 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowdiff is a simple algorithm for evaluating the agreement of two segmentations by running a sliding window over the data and awarding partial credit for near misses. If we preprocess our tokens into a sequence of zeros and ones, to record when a token is followed by a boundary, we can represent the segmentations as strings, and apply the windowdiff scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"00000010000000001000000\"\n",
    "s2 = \"00000001000000010000000\"\n",
    "s3 = \"00010000000000000001000\"\n",
    "nltk.windowdiff(s1, s1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19047619047619047"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.windowdiff(s1, s2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.windowdiff(s2, s3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curation vs Evolution\n",
    "As large corpora are published, researchers are increasingly likely to base their investigations on balanced, focused subsets that were derived from corpora produced for entirely different reasons. For instance, the Switchboard database, originally collected for speaker identification research, has since been used as the basis for published studies in speech recognition, word pronunciation, disfluency, syntax, intonation and discourse structure. The motivations for recycling linguistic corpora include the desire to save time and effort, the desire to work on material available to others for replication, and sometimes a desire to study more naturalistic forms of linguistic behavior than would be possible otherwise. The process of choosing a subset for such a study may count as a non-trivial contribution in itself.\n",
    "\n",
    "In addition to selecting an appropriate subset of a corpus, this new work could involve reformatting a text file (e.g. converting to XML), renaming files, retokenizing the text, selecting a subset of the data to enrich, and so forth. Multiple research groups might do this work independently, as illustrated in 2.2. At a later date, should someone want to combine sources of information from different versions, the task will probably be extremely onerous.\n",
    "\n",
    "Figure 2.2: Evolution of a Corpus over Time\n",
    "![](http://www.nltk.org/images/evolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of using derived corpora is made even more difficult by the lack of any record about how the derived version was created, and which version is the most up-to-date.\n",
    "\n",
    "An alternative to this chaotic situation is for a corpus to be centrally curated, and for committees of experts to revise and extend it at periodic intervals, considering submissions from third-parties, and publishing new releases from time to time. Print dictionaries and national corpora may be centrally curated in this way. However, for most corpora this model is simply impractical.\n",
    "\n",
    "A middle course is for the original corpus publication to have a scheme for identifying any sub-part. Each sentence, tree, or lexical entry, could have a globally unique identifier, and each token, node or field (respectively) could have a relative offset. Annotations, including segmentations, could reference the source using this identifier scheme (a method which is known as standoff annotation). This way, new annotations could be distributed independently of the source, and multiple independent annotations of the same source could be compared and updated without touching the source.\n",
    "\n",
    "If the corpus publication is provided in multiple versions, the version number or date could be part of the identification scheme. A table of correspondences between identifiers across editions of the corpus would permit any standoff annotations to be updated easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Data\n",
    "## Obtaining Data from the web\n",
    "If the desired content is localized to a particular website, there are many utilities for capturing all the accessible contents of a site, such as GNU Wget http://www.gnu.org/software/wget/. For maximal flexibility and control, a web crawler can be used, such as Heritrix http://crawler.archive.org/. Crawlers permit fine-grained control over where to look, which links to follow, and how to organize the results. It might be tempting to write your own web-crawler, but there are dozens of pitfalls to do with detecting MIME types, converting relative to absolute URLs, avoiding getting trapped in cyclic link structures, dealing with network latencies, avoiding overloading the site or being banned from accessing the site, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data from Word Processor Files\n",
    "Consider the following fragment of a lexical entry: \"sleep [sli:p] v.i. condition of body and mind...\". We can enter this in MSWord, then \"Save as Web Page\", then inspect the resulting HTML file:\n",
    "```\n",
    "<p class=MsoNormal>sleep\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  [<span class=SpellE>sli:p</span>]\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
    "</p>\n",
    "```\n",
    "Observe that the entry is represented as an HTML paragraph, using the <p> element, and that the part of speech appears inside a  <span\n",
    "style='font-size:11.0pt'> element. The following program defines the set of legal parts-of-speech, legal_pos. Then it extracts all 11-point content from the dict.htm file and stores it in the set used_pos. Observe that the search pattern contains a parenthesized sub-expression; only the material that matches this sub-expression is returned by re.findall. Finally, the program constructs the set of illegal parts-of-speech as used_pos -\n",
    "legal_pos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
    "document = open(\"dict.htm\", encoding=\"windows-1252\").read()\n",
    "used_pos = set(re.findall(pattern, document))\n",
    "illegal_pos = used_pos.difference(legal_pos)\n",
    "print(list(illegal_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program in 3.1 strips out the HTML markup using nltk.clean_html(), extracts the words and their pronunciations, and generates output in \"comma-separated value\" (CSV) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def lexical_data(html_file, encoding=\"utf-8\"):\n",
    "    SEP = '_ENTRY'\n",
    "    html = open(html_file, encoding=encoding).read()\n",
    "    html = re.sub(r'<p', SEP + '<p', html)\n",
    "    text = BeautifulSoup(html).get_text()\n",
    "    text = ' '.join(text.split())\n",
    "    for entry in text.split(SEP):\n",
    "        if entry.count(' ') > 2:\n",
    "            yield entry.split(' ', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "writer = csv.writer(open(\"dict1.csv\", \"w\", encoding=\"utf-8\"))\n",
    "writer.writerows(lexical_data(\"dict.htm\", encoding=\"windows-1252\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Obtaining Data from Spreadsheets and Databases\n",
    "Spreadsheets are often used for acquiring wordlists or paradigms. For example, a comparative wordlist may be created using a spreadsheet, with a row for each cognate set, and a column for each language. Most spreadsheet software can export their data in CSV \"comma-separated value\" format. \n",
    "\n",
    "When our goal is simply to extract the contents from a database, it is enough to dump out the tables (or SQL query results) in CSV format and load them into our program. Our program might perform a linguistically motivated query which cannot be expressed in SQL, e.g. *select all words that appear in example sentences for which no dictionary entry is provided*. For this task, we would need to extract enough information from a record for it to be uniquely identified, along with the headwords and example sentences. Let's suppose this information was now available in a CSV file dict.csv:\n",
    "```\n",
    "\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
    "\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
    "\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
    "```\n",
    "Now we can express this query as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "lexicon = csv.reader(open('dict.csv'))\n",
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n",
    "lexemes, defns = zip(*pairs)\n",
    "defn_words = set(w for defn in defns for w in defn.split())\n",
    "sorted(defn_words.difference(lexemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data Formats\n",
    "In the simplest case, the input and output formats are isomorphic. For instance, we might be converting lexical data from Toolbox format to XML, and it is straightforward to transliterate the entries one at a time. The structure of the data is reflected in the structure of the required program: a for loop whose body takes care of a single entry.\n",
    "\n",
    "In another common case, the output is a digested form of the input, such as an inverted file index. Here it is necessary to build an index structure in memory, then write it to a file in the desired format. The following example constructs an index that maps the words of a dictionary definition to the corresponding lexeme for each lexical entry, having tokenized the definition text, and discarded short words. Once the index has been constructed we open a file and then iterate over the index entries, to write out the lines in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = nltk.Index((defn_word, lexeme)\n",
    "                 for (lexeme, defn) in pairs\n",
    "                 for defn_word in nltk.word_tokenize(defn)\n",
    "                 if len(defn_word) > 3)\n",
    "with open(\"dict.idx\", \"w\") as idx_file:\n",
    "    for word in sorted(idx):\n",
    "        idx_words = ', '.join(idx[word])\n",
    "        idx_line = \"{}: {}\".format(word, idx_words) [5]\n",
    "        print(idx_line, file=idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file dict.idx contains the following lines. (With a larger dictionary we would expect to find multiple lexemes listed for each index entry.)\n",
    "```\n",
    "body: sleep\n",
    "cease: wake\n",
    "condition: sleep\n",
    "down: walk\n",
    "each: walk\n",
    "foot: walk\n",
    "lifting: walk\n",
    "mind: sleep\n",
    "progress: walk\n",
    "setting: walk\n",
    "sleep: wake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding Which Layers of Annotation to Include\n",
    "Published corpora vary greatly in the richness of the information they contain. At a minimum, a corpus will typically contain at least a sequence of sound or orthographic symbols. At the other end of the spectrum, a corpus could contain a large amount of information about the syntactic structure, morphology, prosody, and semantic content of every sentence, plus annotation of discourse relations or dialogue acts. These extra layers of annotation may be just what someone needs for performing a particular data analysis task. For example, it may be much easier to find a given linguistic pattern if we can search for specific syntactic structures; and it may be easier to categorize a linguistic pattern if every word has been tagged with its sense. Here are some commonly provided annotation layers:\n",
    "\n",
    "Word Tokenization: The orthographic form of text does not unambiguously identify its tokens. A tokenized and normalized version, in addition to the conventional orthographic version, may be a very convenient resource.\n",
    "Sentence Segmentation: As we saw in 3, sentence segmentation can be more difficult than it seems. Some corpora therefore use explicit annotations to mark sentence segmentation.\n",
    "Paragraph Segmentation: Paragraphs and other structural elements (headings, chapters, etc.) may be explicitly annotated.\n",
    "Part of Speech: The syntactic category of each word in a document.\n",
    "Syntactic Structure: A tree structure showing the constituent structure of a sentence.\n",
    "Shallow Semantics: Named entity and coreference annotations, semantic role labels.\n",
    "Dialogue and Discourse: dialogue act tags, rhetorical structure\n",
    "Unfortunately, there is not much consistency between existing corpora in how they represent their annotations. However, two general classes of annotation representation should be distinguished. Inline annotation modifies the original document by inserting special symbols or control sequences that carry the annotated information. For example, when part-of-speech tagging a document, the string \"fly\" might be replaced with the string \"fly/NN\", to indicate that the word fly is a noun in this context. In contrast, standoff annotation does not modify the original document, but instead creates a new file that adds annotation information using pointers that reference the original document. For example, this new document might contain the string  \"<token id=8 pos='NN'/>\", to indicate that token 8 is a noun. (We would want to be sure that the tokenization itself was not subject to change, since it would cause such references to break silently.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standards and Tools\n",
    "For a corpus to be widely useful, it needs to be available in a widely supported format. However, the cutting edge of NLP research depends on new kinds of annotations, which by definition are not widely supported. In general, adequate tools for creation, publication and use of linguistic data are not widely available. Most projects must develop their own set of tools for internal use, which is no help to others who lack the necessary resources. Furthermore, we do not have adequate, generally-accepted standards for expressing the structure and content of corpora. Without such standards, general-purpose tools are impossible — though at the same time, without available tools, adequate standards are unlikely to be developed, used and accepted.\n",
    "\n",
    "One response to this situation has been to forge ahead with developing a generic format which is sufficiently expressive to capture a wide variety of annotation types (see 8 for examples). The challenge for NLP is to write programs that cope with the generality of such formats. For example, if the programming task involves tree data, and the file format permits arbitrary directed graphs, then input data must be validated to check for tree properties such as rootedness, connectedness, and acyclicity. If the input files contain other layers of annotation, the program would need to know how to ignore them when the data was loaded, but not invalidate or obliterate those layers when the tree data was saved back to the file.\n",
    "\n",
    "Another response has been to write one-off scripts to manipulate corpus formats; such scripts litter the filespaces of many NLP researchers. NLTK's corpus readers are a more systematic approach, founded on the premise that the work of parsing a corpus format should only be done once (per programming language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.nltk.org/images/three-layer-arch.png)\n",
    "Instead of focussing on a common format, we believe it is more promising to develop a common interface (cf. nltk.corpus). Consider the case of treebanks, an important corpus type for work in NLP. There are many ways to store a phrase structure tree in a file. We can use nested parentheses, or nested XML elements, or a dependency notation with a (child-id, parent-id) pair on each line, or an XML version of the dependency notation, etc. However, in each case the logical structure is almost the same. It is much easier to devise a common interface that allows application programmers to write code to access tree data using methods such as children(), leaves(), depth(), and so forth. Note that this approach follows accepted practice within computer science, viz. abstract data types, object oriented design, and the three layer architecture (3.2). The last of these — from the world of relational databases — allows end-user applications to use a common model (the \"relational model\") and a common language (SQL), to abstract away from the idiosyncrasies of file storage, and allowing innovations in filesystem technologies to occur without disturbing end-user applications. In the same way, a common corpus interface insulates application programs from data formats.\n",
    "\n",
    "In this context, when creating a new corpus for dissemination, it is expedient to use an existing widely-used format wherever possible. When this is not possible, the corpus could be accompanied with software — such as an nltk.corpus module — that supports existing interface methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with XML\n",
    "The Extensible Markup Language (XML) provides a framework for designing domain-specific markup languages. It is sometimes used for representing annotated text and for lexical resources. Unlike HTML with its predefined tags, XML permits us to make up our own tags. Unlike a database, XML permits us to create data without first specifying its structure, and it permits us to have optional and repeatable elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XML for Linguistic Structures\n",
    "Thanks to its flexibility and extensibility, XML is a natural choice for representing linguistic structures. Here's an example of a simple lexical entry.\n",
    "```\n",
    "(2)\t\t\n",
    "<entry>\n",
    "  <headword>whale</headword>\n",
    "  <pos>noun</pos>\n",
    "  <gloss>any of the larger cetacean mammals having a streamlined\n",
    "    body and breathing through a blowhole on the head</gloss>\n",
    "</entry>\n",
    "```\n",
    "It consists of a series of XML tags enclosed in angle brackets. Each opening tag, like <gloss> is matched with a closing tag, like </gloss>; together they constitute an XML element. The above example has been laid out nicely using whitespace, but it could equally have been put on a single, long line. Our approach to processing XML will usually not be sensitive to whitespace. In order for XML to be well formed, all opening tags must have corresponding closing tags, at the same level of nesting (i.e. the XML document must be a well-formed tree).\n",
    "\n",
    "A further step might be to link our lexicon to some external resource, such as WordNet, using external identifiers. In (4) we group the gloss and a synset identifier inside a new element which we have called \"sense\".\n",
    "```\n",
    "(4)\t\t\n",
    "<entry>\n",
    "  <headword>whale</headword>\n",
    "  <pos>noun</pos>\n",
    "  <sense>\n",
    "    <gloss>any of the larger cetacean mammals having a streamlined\n",
    "      body and breathing through a blowhole on the head</gloss>\n",
    "    <synset>whale.n.02</synset>\n",
    "  </sense>\n",
    "  <sense>\n",
    "    <gloss>a very large person; impressive in size or qualities</gloss>\n",
    "    <synset>giant.n.04</synset>\n",
    "  </sense>\n",
    "</entry>\n",
    "```\n",
    "Alternatively, we could have represented the synset identifier using an XML attribute, without the need for any nested structure, as in (5).\n",
    "```\n",
    "(5)\t\t\n",
    "<entry>\n",
    "  <headword>whale</headword>\n",
    "  <pos>noun</pos>\n",
    "  <gloss synset=\"whale.n.02\">any of the larger cetacean mammals having\n",
    "      a streamlined body and breathing through a blowhole on the head</gloss>\n",
    "  <gloss synset=\"giant.n.04\">a very large person; impressive in size or\n",
    "      qualities</gloss>\n",
    "</entry>\n",
    "```\n",
    "This illustrates some of the flexibility of XML. If it seems somewhat arbitrary that's because it is! Following the rules of XML we can invent new attribute names, and nest them as deeply as we like. We can repeat elements, leave them out, and put them in a different order each time. We can have fields whose presence depends on the value of some other field, e.g. if the part of speech is \"verb\", then the entry can have a past_tense element to hold the past tense of the verb, but if the part of speech is \"noun\" no past_tense element is permitted. To impose some order over all this freedom, we can constrain the structure of an XML file using a \"schema,\" which is a declaration akin to a context free grammar. Tools exist for testing the validity of an XML file with respect to a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Role of XML\n",
    "We can use XML to represent many kinds of linguistic information. However, the flexibility comes at a price. Each time we introduce a complication, such as by permitting an element to be optional or repeated, we make more work for any program that accesses the data. We also make it more difficult to check the validity of the data, or to interrogate the data using one of the XML query languages.\n",
    "\n",
    "Thus, using XML to represent linguistic structures does not magically solve the data modeling problem. We still have to work out how to structure the data, then define that structure with a schema, and then write programs to read and write the format and convert it to other formats. Similarly, we still need to follow some standard principles concerning data normalization. It is wise to avoid making duplicate copies of the same information, so that we don't end up with inconsistent data when only one copy is changed. For example, a cross-reference that was represented as <xref>headword</xref> would duplicate the storage of the headword of some other lexical entry, and the link would break if the copy of the string at the other location was modified. Existential dependencies between information types need to be modeled, so that we can't create elements without a home. For example, if sense definitions cannot exist independently of a lexical entry, the sense element can be nested inside the entry element. Many-to-many relations need to be abstracted out of hierarchical structures. For example, if a word can have many corresponding senses, and a sense can have several corresponding words, then both words and senses must be enumerated separately, as must the list of (word, sense) pairings. This complex structure might even be split across three separate XML files.\n",
    "\n",
    "As we can see, although XML provides us with a convenient format accompanied by an extensive collection of tools, it offers no panacea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ElementTree Interface\n",
    "Python's ElementTree module provides a convenient way to access data stored in XML files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n",
      "<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n",
      "\n",
      "<PLAY>\n",
      "<TITLE>The Merchant of Venice</TITLE>\n"
     ]
    }
   ],
   "source": [
    "merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
    "raw = open(merchant_file).read()\n",
    "print(raw[:163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TITLE>ACT I</TITLE>\n",
      "\n",
      "<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n",
      "<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n",
      "\n",
      "<SPEECH>\n",
      "<SPEAKER>ANTONIO</SPEAKER>\n",
      "<LINE>In sooth, I know not why I am so sad:</LINE>\n"
     ]
    }
   ],
   "source": [
    "print(raw[1789:2006])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just accessed the XML data as a string. As we can see, the string at the start of Act 1 contains XML tags for title, scene, stage directions, and so forth.\n",
    "\n",
    "The next step is to process the file contents as structured XML data, using ElementTree. We are processing a file (a multi-line string) and building a tree, so its not surprising that the method name is parse. The variable merchant contains an XML element PLAY. This element has internal structure; we can use an index to get its first child, a TITLE element. We can also see the text content of this element, the title of the play. To get a list of all the child elements, we use the getchildren() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'PLAY' at 0x10f9e8d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.etree.ElementTree import ElementTree\n",
    "merchant = ElementTree().parse(merchant_file)\n",
    "merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'TITLE' at 0x10f9e8cc8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Merchant of Venice'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'TITLE' at 0x10f9e8cc8>,\n",
       " <Element 'PERSONAE' at 0x10f9e8db8>,\n",
       " <Element 'SCNDESCR' at 0x10f9f4908>,\n",
       " <Element 'PLAYSUBT' at 0x10f9f4958>,\n",
       " <Element 'ACT' at 0x10f9f49a8>,\n",
       " <Element 'ACT' at 0x10fa16c78>,\n",
       " <Element 'ACT' at 0x10faa44f8>,\n",
       " <Element 'ACT' at 0x10fad0a98>,\n",
       " <Element 'ACT' at 0x10faf44f8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT IV\n",
      "<Element 'SCENE' at 0x10fad0b38>\n",
      "SCENE I.  Venice. A court of justice.\n",
      "<Element 'SPEECH' at 0x10faddc28>\n",
      "<Element 'SPEAKER' at 0x10faddc78>\n",
      "PORTIA\n",
      "<Element 'LINE' at 0x10faddcc8>\n",
      "The quality of mercy is not strain'd,\n"
     ]
    }
   ],
   "source": [
    "print(merchant[-2][0].text)\n",
    "print(merchant[-2][1])\n",
    "print(merchant[-2][1][0].text)\n",
    "print(merchant[-2][1][54])\n",
    "print(merchant[-2][1][54][0])\n",
    "print(merchant[-2][1][54][0].text)\n",
    "print(merchant[-2][1][54][1])\n",
    "print(merchant[-2][1][54][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\n",
      "Act 3 Scene 2 Speech 9: Fading in music: that the comparison\n",
      "Act 3 Scene 2 Speech 9: And what is music then? Then music is\n",
      "Act 5 Scene 1 Speech 23: And bring your music forth into the air.\n",
      "Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n",
      "Act 5 Scene 1 Speech 23: And draw her home with music.\n",
      "Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\n",
      "Act 5 Scene 1 Speech 25: Or any air of music touch their ears,\n",
      "Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\n",
      "Act 5 Scene 1 Speech 25: But music for the time doth change his nature.\n",
      "Act 5 Scene 1 Speech 25: The man that hath no music in himself,\n",
      "Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\n",
      "Act 5 Scene 1 Speech 29: It is your music, madam, of the house.\n",
      "Act 5 Scene 1 Speech 32: No better a musician than the wren.\n"
     ]
    }
   ],
   "source": [
    "for i, act in enumerate(merchant.findall('ACT')):\n",
    "    for j, scene in enumerate(act.findall('SCENE')):\n",
    "        for k, speech in enumerate(scene.findall('SPEECH')):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'music' in str(line.text):\n",
    "                    print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PORTIA', 117),\n",
       " ('SHYLOCK', 79),\n",
       " ('BASSANIO', 73),\n",
       " ('GRATIANO', 48),\n",
       " ('ANTONIO', 47)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n",
    "speaker_freq = Counter(speaker_seq)\n",
    "top5 = speaker_freq.most_common(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ANTO BASS GRAT  OTH PORT SHYL \n",
      "ANTO    0   11    4   11    9   12 \n",
      "BASS   10    0   11   10   26   16 \n",
      "GRAT    6    8    0   19    9    5 \n",
      " OTH    8   16   18  153   52   25 \n",
      "PORT    7   23   13   53    0   21 \n",
      "SHYL   15   15    2   26   21    0 \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "abbreviate = defaultdict(lambda: 'OTH')\n",
    "for speaker, _ in top5:\n",
    "    abbreviate[speaker] = speaker[:4]\n",
    "\n",
    "speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ElementTree for Accessing Toolbox Data\n",
    "We can use the toolbox.xml() method to access a Toolbox file and load it into an elementtree object. This file contains a lexicon for the Rotokas language of Papua New Guinea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to access the contents of the lexicon object, by indexes and by paths. Indexes use the familiar syntax, thus lexicon[3] returns entry number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'lx' at 0x10fddb5e8>\n",
      "lx\n",
      "kaa\n"
     ]
    }
   ],
   "source": [
    "print(lexicon[3][0])\n",
    "print(lexicon[3][0].tag)\n",
    "print(lexicon[3][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to access the contents of the lexicon object uses paths. The lexicon is a series of record objects, each containing a series of field objects, such as lx and ps. We can conveniently address all of the lexemes using the path  record/lx. Here we use the findall() function to search for any matches to the path record/lx, and we access the text content of the element, normalizing it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko', 'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', 'kaaova', 'kaapa', 'kaapea', 'kaapie', 'kaapie', 'kaapiepato', 'kaapisi', 'kaapisivira', 'kaapo', 'kaapopato', 'kaara', 'kaare', 'kaareko', 'kaarekopie', 'kaareto', 'kaareva', 'kaava', 'kaavaaua', 'kaaveaka', 'kaaveakapie', 'kaaveakapievira', 'kaaveakavira', 'kae', 'kae', 'kaekae', 'kaekae', 'kaekaearo', 'kaekaeo', 'kaekaesoto', 'kaekaevira', 'kaekeru', 'kaepaa', 'kaepie', 'kaepie', 'kaepievira', 'kaereasi', 'kaereasivira', 'kaetu', 'kaetupie', 'kaetuvira', 'kaeviro', 'kagave', 'kaie', 'kaiea', 'kaikaio', 'kaio', 'kaipori', 'kaiporipie', 'kaiporivira', 'kairi', 'kairiro', 'kairo', 'kaita', 'kaitutu', 'kaitutupie', 'kaitutuvira', 'kakae', 'kakae', 'kakae', 'kakaevira', 'kakapikoa', 'kakapikoto', 'kakapu', 'kakapua', 'kakara', 'kakarapaia', 'kakarau', 'kakarera', 'kakata', 'kakate', 'kakatuara', 'kakau', 'kakauoa', 'kakavea', 'kakavoro', 'kakavu', 'kakeoto', 'kaki', 'kaki', 'kakiaki', 'kakiri', 'kakiua', 'kaku', 'kakua', 'kakuaku', 'kakupaa', 'kakuparei', 'kakupato', 'kakupie', 'kakupute', 'kakutauo', 'kakuto', 'kakutuiato', 'kakuva', 'kakuvira', 'kameoro', 'kandora', 'kaokao', 'kaokaoara', 'kaokaoto', 'kapa', 'kapa', 'kapaava', 'kapai', 'kapara', 'kaparu', 'kaparuvira', 'kapatau', 'kapatoro', 'kapatoroto', 'kape', 'kapeaa', 'kapeaa', 'kapeaavira', 'kapekape', 'kapekapevira', 'kapere', 'kaperepie', 'kapi', 'kapiaa', 'kapiaavira', 'kapikapi', 'kapiro', 'kapiroa', 'kapiroko', 'kapisi', 'kapisito', 'kapiu', 'kapiua', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapoa', 'kapokapora', 'kapokaporo', 'kapokari', 'kapokarito', 'kapokoa', 'kapoo', 'kapooto', 'kapoovira', 'kapopaa', 'kaporo', 'kaporo', 'kaporopa', 'kaporoto', 'kapoto', 'kapu', 'kapua', 'kapua', 'kapuapato', 'kapuapie', 'kapuasisi', 'kapupie', 'kapupiea', 'kapupiepaa', 'kapuu', 'kapuupie', 'kapuupiepa', 'kara', 'kara', 'karaava', 'karakarao', 'karakaraoa', 'karakaraoto', 'karakaraovira', 'karakaroto', 'karakuku', 'karaova', 'karapi', 'karapivira', 'karara', 'karata', 'karato', 'karavau', 'karavisi', 'karavisito', 'karavuru', 'kare', 'kare', 'karekare', 'karekare', 'karekarererava', 'karekareto', 'kareke', 'karekepie', 'karekova', 'kareo', 'kareovira', 'karepie', 'karepieto', 'karepirie', 'kari', 'karia', 'kariava', 'karikari', 'karirapa', 'karisito', 'karivai', 'karivaito', 'karivara', 'karo', 'karokaropo', 'karopato', 'karopo', 'karot', 'karoto', 'karova', 'karu', 'karuka', 'karukaru', 'karukava', 'karuru', 'karutu', 'karuvira', 'kasi', 'kasi', 'kasi', 'kasiarao', 'kasiava', 'kasikasi', 'kasikasiua', 'kasipie', 'kasipu', 'kasipupie', 'kasipuvira', 'kasirao', 'kasiraopie', 'kasiraovira', 'kasiu', 'kasiura', 'kasivari', 'kasuari', 'kata', 'katai', 'kataitoarei', 'katakatai', 'katakataivira', 'kataraua', 'katarauto', 'katavira', 'katokato', 'katokatoto', 'katokatovira', 'katokoi', 'katopato', 'katoto', 'katuara', 'katuarato', 'katukatu', 'katuta', 'kau', 'kaukau', 'kaukaupie', 'kaukauvira', 'kaukovo', 'kauo', 'kauokauo', 'kaureo', 'kaureoto', 'kausiopa', 'kausiovira', 'kava', 'kavakavau', 'kavatao', 'kavatara', 'kavau', 'kavau', 'kavau asiava', 'kave', 'kavee', 'kaveepaa', 'kaverui', 'kaveruko', 'kavesi', 'kavikavi', 'kavikaviru', 'kavikaviru', 'kaviko', 'kavikoa', 'kaviru', 'kaviru', 'kaviruto', 'kaviruvira', 'kavo', 'kavokavo', 'kavokavoa', 'kavokavoto', 'kavora', 'kavorato', 'kavori', 'kavori', 'kavorou', 'kavovoa', 'kavovovira', 'kavu', 'kavu', 'kavuava', 'kavupie', 'kavura', 'kavurao', 'kavusi', 'kavuvo', 'kea', 'keakea', 'keakeato', 'keari', 'kearito', 'keavira', 'kee', 'keekee', 'keekeepa', 'keekeeri', 'keekeerito', 'keera', 'keeriva', 'keesi', 'keetaa', 'keevuru', 'keevuruvira', 'kegi', 'kei', 'keke', 'keke', 'kekepie', 'kekeputu', 'kekeputuvira', 'kekeraokovira', 'kekesopa', 'kekevoto', 'kekevotovira', 'kekira', 'keo', 'keoka', 'keopa', 'keovira', 'kepa', 'kepa toupato', 'kepetai', 'kepi', 'kepia', 'kepikepi', 'kepiko', 'kepiriko', 'kepiro', 'kepisi', 'kepisiva', 'kepita', 'kepitai', 'kepito', 'kepo', 'kepoi', 'keposi', 'kepoto', 'kera', 'kerakera', 'kerari', 'keraria', 'kerau', 'kerauto', 'keravisi', 'keravisia', 'keravo', 'kereaka', 'kerekoi', 'kerere', 'kerereua', 'kerete', 'kerete', 'kerevaru', 'keri', 'keriaka', 'kerikerisi', 'kerio', 'kerioua', 'keripaara', 'keripato', 'kerisi', 'kerisito', 'kerisivira', 'keritara', 'keriva', 'keroroi', 'kerosiri', 'keru', 'keru', 'kerui', 'keruiato', 'keruito', 'kerupiua', 'keruria', 'keruriato', 'keruriavira', 'kesi', 'kesie', 'kesievira', 'kesikea vaaguru', 'kesio', 'kesioto', 'kesivira', 'keta', 'ketaka', 'ketakaa', 'ketato', 'ketoo', 'ketoo', 'ketoopie', 'ketoopieara', 'ketoroa', 'ketu', 'kevaita', 'kevaita', 'kevaitato', 'kevira', 'kevisi', 'kevoisi', 'kevoisivira', 'kie', 'kii', 'kiikariko', 'kiipie', 'kiire', 'kiire', 'kiiru', 'kiiuto', 'kiki', 'kiki', 'kikipi', 'kikipisi', 'kikira', 'kikiraeko', 'kikisi', 'kikisikae', 'kikisiova', 'kikitausi', 'kikoo', 'kilia', 'kio', 'kipa', 'kipapie', 'kipe', 'kipekipe', 'kipekipea', 'kipeto', 'kipu', 'kipukipu', 'kipupaa', 'kipupato', 'kipuvira', 'kira', 'kirava', 'kire', 'kiri', 'kirikaokao', 'kirioto', 'kiro', 'kirokiro', 'kirokiro', 'kirokiropato', 'kiroko', 'kiru', 'kirukiru', 'kirukirua', 'kirupato', 'kitoiva', 'kitu', 'kitukitu', 'kiu', 'kiupie', 'kiuto', 'kiuve', 'kiuvu', 'koa', 'koai', 'koakoa', 'koakoa', 'koara', 'koarao', 'koaraua', 'koarava', 'koasio', 'koata', 'koatapie', 'koauve', 'koavaato', 'koe', 'koea', 'koekoe', 'koekoeto', 'koepato', 'koeta', 'koetaova', 'koetapie', 'koetava', 'koetavira', 'koeto', 'kogo', 'kogova', 'koi', 'koie', 'koie', 'koike', 'koiketo', 'koikevira', 'koikoi', 'koikoipato', 'koikoipie', 'koikoito', 'koisi', 'koisiva', 'koivira', 'koka', 'kokai', 'kokara', 'kokaraa', 'kokarapato', 'koke', 'kokee', 'kokepato', 'kokepato', 'kokepie', 'kokerao', 'kokeriva', 'kokeu', 'kokeva', 'koki', 'kokio', 'kokipaia', 'kokito', 'kokivira', 'koko', 'koko', 'kokoi', 'kokoisi', 'kokokoru', 'kokoo', 'kokooko', 'kokookoa', 'kokoote', 'kokootu', 'kokopa', 'kokopakou', 'kokopeko', 'kokopekovira', 'kokopeoto', 'kokopuoto', 'kokopuovira', 'kokopuru', 'kokopuvira', 'kokora', 'kokorai', 'kokorato', 'kokori', 'kokorivira', 'kokoro', 'kokoroki', 'kokoroku', 'kokorokupie', 'kokoropato', 'kokorosi', 'kokorovira', 'kokoru', 'kokoruu', 'kokoruu', 'kokosi', 'kokosi', 'kokosiria', 'kokosito', 'kokosiva', 'kokotagoe', 'kokote', 'kokoto', 'kokotu', 'kokotua', 'kokotuo', 'kokovae', 'kokovaeva', 'kokovara', 'kokovara', 'kokovaravira', 'kokovu', 'kokovua', 'kokovua', 'kokovua', 'kokovupaparie', 'kokovurito', 'koku', 'kokuoku', 'kokureko', 'kokuuto', 'kooe', 'kookaa', 'kookai', 'kookoo', 'kookooia', 'kookoopeko', 'kookoopi', 'kooku', 'kookuto', 'kookuvira', 'koopi', 'koopipi', 'koora', 'kooroo', 'koorooto', 'koorooto', 'kooroovira', 'kooru', 'koota', 'kootopa', 'kootutu', 'koou', 'kooupato', 'koova', 'koova', 'koovoto', 'koovotova', 'kopa', 'kopakai', 'kopakava', 'kopakopa', 'kopakovira', 'kopato', 'kopii', 'kopiia', 'kopiipato', 'kopiipie', 'kopiito', 'kopikao', 'kopikopi', 'kopikopiara', 'kopipi', 'kopirovu', 'kopu', 'kopua', 'kopuasi', 'kopuasipie', 'kopuasito', 'kopuasitovira', 'kopuasivira', 'kopuisi', 'kopukopu', 'kopukopua', 'kopupa', 'kopupira', 'kopuro', 'kopuru', 'kopuvioro', 'kopuvira', 'kora', 'korapato', 'korara', 'koraraoko', 'korau', 'koraua', 'korauru', 'korauvira', 'korea', 'korekare', 'korere', 'korereto', 'kori', 'koria', 'koribori', 'korikori', 'korikoripava', 'korita', 'korita', 'koriteira', 'koro', 'kororo', 'kororo', 'kororoisivira', 'kororovi', 'kororovivira', 'kororu', 'kororupie', 'koroto', 'koroviri', 'korovo', 'koru', 'koruko', 'korukoru', 'korukorupato', 'koruo', 'koruoto', 'koruou', 'koruoua', 'koruovira', 'korupie', 'korupievira', 'kosi', 'kosikosi', 'kosikosi', 'kosipa', 'kosipato', 'kosipie', 'kosivago', 'kosiviro', 'koto', 'kotokoto', 'kotokotoara', 'kotopa', 'kotopa', 'kotovira', 'kotu', 'kotukotu', 'kotupiua', 'koturu', 'kou', 'kou', 'koue', 'koue', 'koui', 'koukou', 'koukouo', 'koukouo', 'kova', 'kovaaro', 'kovaeto', 'kovaii', 'kovakovara', 'kovapato', 'kovarato', 'kovarua', 'kovasi', 'kovata', 'kovatavira', 'kovato', 'kovauke', 'kovava', 'kove', 'kove', 'kovea', 'kovekove', 'koveoapa', 'koveva', 'kovia', 'kovikoro', 'kovire', 'kovirea', 'kovo', 'kovo', 'kovo', 'kovoa', 'kovokovo', 'kovokovo', 'kovokovo', 'kovokovoa', 'kovopaa', 'kovopato', 'kovopie', 'kovoruko', 'kovoto', 'kovovo', 'kovuaka', 'kovuaro', 'kovukovu', 'kovukovuto', 'kovukovuvira', 'kovuru', 'kovurui', 'kovuruko', 'kovurukovira', 'kovuruvira', 'kovuto', 'kuara', 'kue', 'kuea', 'kuga', 'kui', 'kuio', 'kuioi', 'kuiopesi', 'kuiopetu', 'kuito', 'kuka', 'kukara', 'kukauviro', 'kukiuki', 'kukiukia', 'kuku', 'kukua', 'kukuaua', 'kukue', 'kukue pute', 'kukuepaa', 'kukuku', 'kukupira', 'kukuriko', 'kukurikoto', 'kukusiri', 'kukutauvu', 'kukutu', 'kukuuku', 'kukuukua', 'kukuukupie', 'kukuuvua', 'kukuvai', 'kukuvaipaa', 'kukuvita', 'kukuvua', 'kuokuo', 'kupare', 'kupareto', 'kupekupe', 'kupekupepa', 'kupero', 'kuperoo', 'kuperovira', 'kupii', 'kupukupu', 'kurae', 'kurasia', 'kurasia', 'kurei', 'kuri', 'kuriato', 'kurikaakaakuto', 'kurikasi', 'kurikasivira', 'kurikoko', 'kurikuri', 'kuripaa', 'kuritava', 'kuroa', 'kuroea', 'kurokuro', 'kurokuroto', 'kuroo', 'kurooro', 'kurovira', 'kuru', 'kurue', 'kurupi', 'kururai', 'kururu', 'kurutu', 'kusi', 'kusii', 'kusike', 'kusito', 'kuu', 'kuu', 'kuukuuvu', 'kuukuuvuto', 'kuuoa', 'kuupie', 'kuurea', 'kuuri', 'kuuva', 'kuuvaki', 'kuuvakito', 'kuuvato', 'kuuvavira', 'kuuvu', 'kuuvuvira', 'kuva', 'kuvaku', 'kuvato', 'kuvau', 'kuvaupie', 'kuvauvira', 'kuvera', 'kuverava', 'kuvere', 'kuverea', 'kuvereto', 'kuverevira', 'kuvoro', 'kuvu', 'kuvuara', 'kuvukuvu', 'kuvukuvua', 'kuvupato', 'kuvuto']\n"
     ]
    }
   ],
   "source": [
    "print([lexeme.text.lower() for lexeme in lexicon.findall('record/lx')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the Toolbox data in XML format. The write() method of ElementTree expects a file object. We usually create one of these using Python's built-in open() function. In order to see the output displayed on the screen, we can use a special pre-defined file object called stdout (standard output), defined in Python's sys module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<record>\n",
      "    <lx>kaa</lx>\n",
      "    <ps>N</ps>\n",
      "    <pt>MASC</pt>\n",
      "    <cl>isi</cl>\n",
      "    <ge>cooking banana</ge>\n",
      "    <tkp>banana bilong kukim</tkp>\n",
      "    <pt>itoo</pt>\n",
      "    <sf>FLORA</sf>\n",
      "    <dt>12/Aug/2005</dt>\n",
      "    <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n",
      "    <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n",
      "    <xe>Taeavi planted banana in order to cook it.</xe>\n",
      "  </record>"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nltk.util import elementtree_indent\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "elementtree_indent(lexicon)\n",
    "tree = ElementTree(lexicon[3])\n",
    "tree.write(sys.stdout, encoding='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Entries\n",
    "We can use the same idea we saw above to generate HTML tables instead of plain text. This would be useful for publishing a Toolbox lexicon on the web. It produces HTML elements `<table>`, `<tr>` (table row), and `<td>` (table data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "  <tr><td>kakae</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakae</td><td>CLASS</td><td>child</td></tr>\n",
      "  <tr><td>kakaevira</td><td>ADV</td><td>small-like</td></tr>\n",
      "  <tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakapikoto</td><td>N</td><td>newborn baby</td></tr>\n",
      "  <tr><td>kakapu</td><td>V</td><td>place in sling for purpose of carrying</td></tr>\n",
      "  <tr><td>kakapua</td><td>N</td><td>sling for lifting</td></tr>\n",
      "  <tr><td>kakara</td><td>N</td><td>arm band</td></tr>\n",
      "  <tr><td>Kakarapaia</td><td>N</td><td>village name</td></tr>\n",
      "  <tr><td>kakarau</td><td>N</td><td>frog</td></tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "html = \"<table>\\n\"\n",
    "for entry in lexicon[70:80]:\n",
    "    lx = entry.findtext('lx')\n",
    "    ps = entry.findtext('ps')\n",
    "    ge = entry.findtext('ge')\n",
    "    html += \"  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (lx, ps, ge)\n",
    "html += \"</table>\"\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Toolbox Data\n",
    "Given the popularity of Toolbox amongst linguists, we will discuss some further methods for working with Toolbox data. Many of the methods discussed in previous chapters, such as counting, building frequency distributions, tabulating co-occurrences, can be applied to the content of Toolbox entries. For example, we can trivially compute the average number of fields for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.635955056179775"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')\n",
    "sum(len(entry) for entry in lexicon) / len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Field to Each Entry\n",
    "It is often convenient to add new fields that are derived automatically from existing ones. Such fields often facilitate search and analysis. For instance, we define a function cv() which maps a string of consonants and vowels to the corresponding CV sequence, e.g. kakapua would map to CVCVCVV. This mapping has four steps. First, the string is converted to lowercase, then we replace any non-alphabetic characters [^a-z] with an underscore. Next, we replace all vowels with V. Finally, anything that is not a V or an underscore must be a consonant, so we replace it with a C. Now, we can scan the lexicon and add a new cv field after every lx field. The following example shows what this does to a particular entry; note the last line of output, which shows the new cv field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import SubElement\n",
    "\n",
    "def cv(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z]',     r'_', s)\n",
    "    s = re.sub(r'[aeiou]',    r'V', s)\n",
    "    s = re.sub(r'[^V_]',      r'C', s)\n",
    "    return (s)\n",
    "\n",
    "def add_cv_field(entry):\n",
    "    for field in entry:\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = SubElement(entry, 'cv')\n",
    "            cv_field.text = cv(field.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaeviro\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\\cv CVVCVCV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')\n",
    "add_cv_field(lexicon[53])\n",
    "print(nltk.toolbox.to_sfm_string(lexicon[53]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating a Toolbox Lexicon\n",
    "Many lexicons in Toolbox format do not conform to any particular schema. Some entries may include extra fields, or may order existing fields in a new way. Manually inspecting thousands of lexical entries is not practicable. However, we can easily identify frequent field sequences, with the help of a Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 17),\n",
       " ('lx:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 16),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 12),\n",
       " ('lx:ps:pt:ge:tkp:nt:sf:dt:ex:xp:xe', 9),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:ex:xp:xe', 9),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 9),\n",
       " ('lx:ps:ge:tkp:dt:ex:xp:xe', 8),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 8),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe', 8),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 7),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 7),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 6),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt:ex:xp:xe', 5),\n",
       " ('lx:ps:pt:ge:tkp:nt:sf:dt:ex:xp:xe:ex:xp:xe', 5),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:dt:ex:xp:xe', 5),\n",
       " ('lx:rt:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 4),\n",
       " ('lx:ps:pt:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe', 4),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 4),\n",
       " ('lx:ps:pt:ge:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 4),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:dt:ex:xp:xe', 4),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt', 4),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 4),\n",
       " ('lx:ps:pt:ge:tkp:eng:dt', 4),\n",
       " ('lx:ps:pt:ge:tkp:vx:cmt:dt:ex:xp:xe', 4),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:eng:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:vx:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:dx:ge:tkp:dt:ex:xp:xe', 3),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:cmt:vx:dt:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:vx:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:dt:cmt:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:nt:sf:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe', 3),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:sf:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:nt:sf:dt', 2),\n",
       " ('lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:arg:vx:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:eng:ig:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:sf:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:avm:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:eng:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:dx:ge:tkp:eng:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:cmt:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:avm:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:dt:ex:xp:xe', 2),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:nt:ig:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:ge:tkp:tkp:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:cmt:vx:dt', 2),\n",
       " ('lx:rt:ps:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:dx:ge:tkp:vx:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:ge:tkp:dt', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:nt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:dx:ge:tkp:eng:nt:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:cmt:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:sc:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:dx:ge:ge:tkp:avm:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:cm:arg:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:ge:tkp:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:sc:vx:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:ge:tkp:eng:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:sf:nt:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:eng:sa:sf:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:dx:dt:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:cmt:dt', 2),\n",
       " ('lx:ps:pt:ge:tkp:nt:pt:dt:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:avm:dt:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 2),\n",
       " ('lx:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe', 2),\n",
       " ('_sh:_DateStampHasFourDigitYear', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:sc:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:cl:ge:tkp:pt:sf:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:rdp:ge:tkp:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sf:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:eng:ge:tkp:tkp:arg:vx:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:arg:vx:dcsv:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:eng:eng:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sf:nt:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:dcsv:dt:cmt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:eng:eng:eng:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:dcsv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:cmt:dt', 1),\n",
       " ('lx:rt:ps:ge:tkp:am:dcsv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:ge:ge:ge:tkp:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:ge:ge:tkp:dcsv:am:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:sc:vx:nt:dt:ex:xp:xe:cv', 1),\n",
       " ('lx:ps:pt:ge:tkp:sf:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:cmt:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:dx:ge:eng:eng:tkp:tkp:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:nt:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sf:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:ge:ge:tkp:cmt:dcsv:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:cmt:arg:vx:cmt:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:ge:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:cl:ge:tkp:nt:sf:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:cmt:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:nt:dcsv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:arg:vx:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:dx:ge:tkp:avm:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:vx:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:tkp:nt:sa:sf:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:ig:nt:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:tkp:ig:nt:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:vx:arg:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:arg:vx:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:cm:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:dx:ge:tkp:eng:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:sf:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:eng:eng:eng:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:dx:ge:eng:eng:tkp:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:sa:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:eng:eng:tkp:dt', 1),\n",
       " ('lx:ps:pt:ge:eng:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:arg:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dcsv:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:nt:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:rdp:ge:tkp:cmt:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:eng:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sc:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:arg:vx:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:dcsv:vx:sc:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:rdp:ge:tkp:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:rdp:ge:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:dx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:dcsv:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:dx:dx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ig:ge:tkp:dt', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:dcpv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:ig:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:nt:vx:sc:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:arg:vx:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:eng:tkp:ge:vx:arg:dt:rdp:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:avm:sc:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:nt:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:tkp:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:ge:tkp:arg:vx:cm:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:nt:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:tkp:eng:ig:dt', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:sc:dcpv:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:dt:alt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:tkp:eng:eng:eng:eng:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:tkp:dcsv:cm:vx:arg:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:tkp:vx:sc:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:tkp:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:ig:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcpv:vx:arg:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:dx:ge:tkp:eng:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:eng:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:ge:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:sc:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:ge:tkp:tkp:nt:vx:sc:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:ge:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:nt:sa:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt:alt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:sc:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:eng:eng:eng:eng:cm:am:dcsv:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:alt:ps:pt:ge:ge:ge:ge:tkp:cmt:cm:arg:vx:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:nt:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:eng:eng:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:arg:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:cm:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cm:vx:arg:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:cmt:arg:vx:cm:sa:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:tkp:vx:sc:dcpv:dt:cmt:ex:xp:xe:ex:xp:xp', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:tkp:vx:arg:sc:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:vx:arg:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:cm:arg:cmt:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:eng:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:nt:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:tkp:cm:vx:arg:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:ge:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:nt:dt', 1),\n",
       " ('lx:rt:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:vx:arg:cmt:sc:dcpv:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:dcpv:dadv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:dx:ge:tkp:dt:nt:cmt', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:am:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:rt:ps:pt:dx:ge:tkp:dt', 1),\n",
       " ('lx:rt:ps:pt:cl:ge:tkp:sf:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:arg:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:sa:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:ge:tkp:dt:cmt', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:cm:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:sa:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:wf:wf:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:nt:dt', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:vx:nt:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:ge:tkp:dt', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dx:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:cmt:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:ge:tkp:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:cmt:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:vx:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:cmt:dcsv:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:ex:xp:xe:dt', 1),\n",
       " ('lx:ps:pt:dx:ge:tkp:nt:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:cl:ps:pt:ge:tkp:dt', 1),\n",
       " ('lx:ps:pt:dx:ge:eng:eng:tkp:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:arg:vx:cmt:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:dx:ge:tkp:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:sf:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:avm:sc:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:vx:arg:cmt:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:tkp:dcsv:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:arg:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:sa:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:sa:cmt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:rdp:arg:vx:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:avm:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:dcsv:vx:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:eng:tkp:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sc:vx:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:eng:eng:eng:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:dcsv:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:eng:ge:tkp:arg:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:cm:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:ex:xp:xe:dt', 1),\n",
       " ('lx:rt:ps:pt:dx:ge:tkp:eng:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:eng:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:pt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:tkp:eng:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:nt:sa:sf:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:dt:sc:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:tkp:ig:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:sf:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:eng:eng:am:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:cmt:vx:dcsv:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:eng:eng:eng:avm:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:vx:dt:ex:xp', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:ex', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:pt:sa:sa:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:cmt', 1),\n",
       " ('lx:ps:pt:ge:tkp:sc:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:tkp:sc:cmt:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:cl:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:dx:ge:tkp:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:sn:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:ge:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:sf:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:cmt:cm:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:sn:ge:tkp:nt:sf:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:sn:ge:tkp:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sc:nt:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:eng:eng:am:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:cmt:dcsv:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:nt:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:vx:dx:ge:eng:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:nt:dt', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:tkp:nt:dt:cmt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:am:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:cl:ge:tkp:pt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:tkp:vx:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:nt:sa:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:eng:tkp:vx:cmt:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:tkp:eng:eng:eng:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:ge:ge:ge:nt:arg:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:cmt:dt:vx:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:cl:ge:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dcsv:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cm:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:arg:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:sa:dx:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:rdp:ps:pt:ge:ge:eng:eng:eng:eng:tkp:vx:arg:cm:dt:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:avm:dt', 1),\n",
       " ('lx:ps:ge:tkp:cmt:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:cm:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:eng:tkp:tkp:dcsv:sc:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:rdp:ps:pt:ge:tkp:tkp:eng:eng:ge:tkp:dcpv:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:tkp:vx:arg:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:cmt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:arg:vx:dt:rdp:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:wf:rt:ps:pt:ge:tkp:eng:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dcsv:am:arg:sc:vx:nt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:sa:sc:vx:arg:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:sa:sc:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:pt:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:sc:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:dt:cmt:sc:vx:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:sc:dt:rdp:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:tkp:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:nt:dt', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:eng:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dcsv:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:ge:ge:tkp:tkp:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:tkp:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:rt:ps:pt:ge:eng:tkp:vx:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:eng:eng:tkp:arg:vx:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:eng:eng:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:ge:ge:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:alt:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:alt:ps:pt:ge:ge:tkp:am:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:alt:rt:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:cmt:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:cm:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:pt:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:tkp:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:arg:vx:nt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:eng:eng:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:nt:sa:dt', 1),\n",
       " ('lx:ps:ge:ge:tkp:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:nt:cmt:sa:sf:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:dt:vx:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:vx:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:cmt:vx:arg:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:cl:ge:tkp:nt:sa:sf:dt:cmt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:eng:eng:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:vx:sc:dt:rdp:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:tkp:dt', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:arg:vx:dt:rdp:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:sc:dt:rdp:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:arg:vx:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:eng:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:eng:nt:sa:sf:dt', 1),\n",
       " ('lx:ps:pt:ge:tkp:dt:vx:sc:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:sa:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:eng:eng:tkp:dcsv:vx:cmt:dt', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:tkp:cmt:vx:arg:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:vx:sc:dt:cmt:rdp:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:nt:cmt:sf:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:eng:eng:arg:vx:cmt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe',\n",
       "  1),\n",
       " ('lx:ps:pt:ge:ge:tkp:vx:sc:cmt:dt:cmt:ex:xp:xe', 1),\n",
       " ('lx:ps:ge:ge:tkp:tkp:dt:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:vx:sc:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:tkp:cmt:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:ge:ge:tkp:cm:arg:vx:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:cmt:arg:vx:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:ge:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:ge:tkp:eng:eng:cmt:dt:vx:sc:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:ps:pt:ge:tkp:tkp:eng:eng:eng:arg:vx:dt:cmt:ex:xp:xe:ex:xp:xe', 1),\n",
       " ('lx:rt:ps:pt:rdp:ge:ge:tkp:vx:arg:cmt:dt:ex:xp:xe', 1),\n",
       " ('lx:alt:rt:ps:pt:ge:eng:eng:eng:tkp:tkp:sc:dt:ex:xp:xe', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "field_sequences = Counter(':'.join(field.tag for field in entry) for entry in lexicon)\n",
    "field_sequences.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting these field sequences we could devise a context free grammar for lexical entries. The following grammar uses the CFG format. Such a grammar models the implicit nested structure of Toolbox entries, and builds a tree structure in which the leaves of the tree are individual field names. Finally, we iterate over the entries and report their conformance with the grammar. Those that are accepted by the grammar are prefixed with a '+', and those that are rejected are prefixed with a '-'. During the process of developing such a grammar it helps to filter out some of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring('''\n",
    "  S -> Head PS Glosses Comment Date Sem_Field Examples\n",
    "  Head -> Lexeme Root\n",
    "  Lexeme -> \"lx\"\n",
    "  Root -> \"rt\" |\n",
    "  PS -> \"ps\"\n",
    "  Glosses -> Gloss Glosses |\n",
    "  Gloss -> \"ge\" | \"tkp\" | \"eng\"\n",
    "  Date -> \"dt\"\n",
    "  Sem_Field -> \"sf\"\n",
    "  Examples -> Example Ex_Pidgin Ex_English Examples |\n",
    "  Example -> \"ex\"\n",
    "  Ex_Pidgin -> \"xp\"\n",
    "  Ex_English -> \"xe\"\n",
    "  Comment -> \"cmt\" | \"nt\" |\n",
    "  ''')\n",
    "\n",
    "def validate_lexicon(grammar, lexicon, ignored_tags):\n",
    "    rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "    for entry in lexicon:\n",
    "        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n",
    "        if list(rd_parser.parse(marker_list)):\n",
    "            print(\"+\", ':'.join(marker_list))\n",
    "        else:\n",
    "            print(\"-\", ':'.join(marker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:sf:dt\n",
      "- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\n",
      "- lx:rt:ps:ge:ge:tkp:dt\n",
      "- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:dt:ex:xp:xe\n",
      "- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\n"
     ]
    }
   ],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')[10:20]\n",
    "ignored_tags = ['arg', 'dcsv', 'pt', 'vx']\n",
    "validate_lexicon(grammar, lexicon, ignored_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to use a chunk parser, since these are much more effective at identifying partial structures, and can report the partial structures that have been identified. In 5.3 we set up a chunk grammar for the entries of a lexicon, then parse each entry. A sample of the output from this program is shown in 5.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 5.3\n",
    "grammar = r\"\"\"\n",
    "      lexfunc: {<lf>(<lv><ln|le>*)*}\n",
    "      example: {<rf|xv><xn|xe>*}\n",
    "      sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\n",
    "      record:   {<lx><hm><sense>+<dt>}\n",
    "    \"\"\"\n",
    "\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "from nltk.toolbox import ToolboxData\n",
    "db = ToolboxData()\n",
    "db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))\n",
    "lexicon = db.parse(grammar, encoding='utf8')\n",
    "tree = ElementTree(lexicon)\n",
    "with open(\"iu_mien_samp.xml\", \"wb\") as output:\n",
    "    tree.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5.4: XML Representation of a Lexical Entry, Resulting from Chunk Parsing a Toolbox Record\n",
    "![](http://www.nltk.org/images/iu-mien.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Fundamental data types, present in most corpora, are annotated texts and lexicons. Texts have a temporal structure, while lexicons have a record structure.\n",
    "- The lifecycle of a corpus includes data collection, annotation, quality control, and publication. The lifecycle continues after publication as the corpus is modified and enriched during the course of research.\n",
    "- Corpus development involves a balance between capturing a representative sample of language usage, and capturing enough material from any one source or genre to be useful; multiplying out the dimensions of variability is usually not feasible because of resource limitations.\n",
    "- XML provides a useful format for the storage and interchange of linguistic data, but provides no shortcuts for solving pervasive data modeling problems.\n",
    "- Toolbox format is widely used in language documentation projects; we can write programs to support the curation of Toolbox files, and to convert them to XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
