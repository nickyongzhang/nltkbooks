{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammatical Features\n",
    " In this chapter, we will investigate the role of features in building rule-based grammars. In contrast to feature extractors, which record features that have been automatically detected, we are now going to declare the features of words and phrases. We start off with a very simple example, using dictionaries to store features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kim = {'CAT': 'NP','ORTH':'Kim','REF':'k'}\n",
    "chase = {'CAT': 'V','ORTH':'chased','REL':'chase'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects kim and chase both have a couple of shared features, CAT (grammatical category) and ORTH (orthography, i.e., spelling). In addition, each has a more semantically-oriented feature: kim['REF'] is intended to give the referent of kim, while chase['REL'] gives the relation expressed by chase. In the context of rule-based grammars, such pairings of features and values are known as **feature structures**.\n",
    "\n",
    "Feature structures contain various kinds of information about grammatical entities. The information need not be exhaustive, and we might want to add further properties. For example, in the case of a verb, it is often useful to know what \"semantic role\" is played by the arguments of the verb. In the case of chase, the subject plays the role of \"agent\", while the object has the role of \"patient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chase['AGT'] = 'sbj'\n",
    "chase['PAT'] = 'obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n"
     ]
    }
   ],
   "source": [
    "sent = 'Kim chased Lee'\n",
    "tokens = sent.split()\n",
    "lee = {'CAT':'NP','ORTH':'Lee','REF':'l'}\n",
    "def lex2fs(word):\n",
    "    for fs in [kim,lee,chase]:\n",
    "        if fs['ORTH'] == word:\n",
    "            return fs\n",
    "        \n",
    "subj, verb, obj = lex2fs(tokens[0]),lex2fs(tokens[1]),lex2fs(tokens[2])\n",
    "verb['AGT'] = subj['REF']\n",
    "verb['PAT'] = obj['REF']\n",
    "for k in ['ORTH','REL','AGT','PAT']:\n",
    "    print('%-5s => %s' % (k,verb[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach could be adopted for a different verb, say surprise, though in this case, the subject would play the role of \"source\" (SRC) and the object, the role of \"experiencer\" (EXP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise = {'CAT':'V','ORTH':'surprised','REL':'surprise',\n",
    "           'SRC':'sbj','EXP':'obj'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That morphological properties of the verb co-vary with syntactic properties of the subject noun phrase. This co-variance is called **agreement**. If we look further at verb agreement in English, we will see that present tense verbs typically have two inflected forms: one for third person singular, and another for every other combination of person and number, as shown in Table below.\n",
    "\n",
    "*Agreement Paradigm for English Regular Verbs*\n",
    "\n",
    "| |singular|plural|\n",
    "|-|------|------|\n",
    "|1st per|I run|we run|\n",
    "|2nd per|you run|you run|\n",
    "|3rd per|he/she/it runs|they run|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "S   ->   NP VP\n",
    "NP  ->   Det N\n",
    "VP  ->   V\n",
    "\n",
    "Det  ->  'this'\n",
    "N    ->  'dog'\n",
    "V    ->  'runs'\n",
    "```\n",
    "\n",
    "To\n",
    "```\n",
    "S -> NP_SG VP_SG\n",
    "S -> NP_PL VP_PL\n",
    "NP_SG -> Det_SG N_SG\n",
    "NP_PL -> Det_PL N_PL\n",
    "VP_SG -> V_SG\n",
    "VP_PL -> V_PL\n",
    "\n",
    "Det_SG -> 'this'\n",
    "Det_PL -> 'these'\n",
    "N_SG -> 'dog'\n",
    "N_PL -> 'dogs'\n",
    "V_SG -> 'runs'\n",
    "V_PL -> 'run'\n",
    "```\n",
    "The change results in the sentences generated by grammar can involve both singular subject NPS and VPS, and plural subject NPS and VPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Attributes and Constraits\n",
    "```\n",
    "S -> NP[NUM=?n] VP[NUM=?n]\n",
    "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
    "VP[NUM=?n] -> V[NUM=?n]\n",
    "```\n",
    "\n",
    "We have introduced some new notation which says that the category N has a (grammatical) **feature** called NUM (short for 'number'), We are using ?n as a variable over values of NUM; it can be instantiated either to sg or pl, within a given production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a syntactic category can have more than one feature; for example, V[TENSE=pres, NUM=pl]. In general, we can add as many features as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'Kim likes children'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat0.fcfg',trace=2)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "So far, we have only seen feature values like sg and pl. These simple values are usually called **atomic** — that is, they can't be decomposed into subparts. A special case of atomic values are **boolean** values, that is, values that just specify whether a property is true or false. For example, we might want to distinguish **auxiliary** verbs such as can, may, will and do with the boolean feature AUX. For example, the production V[TENSE=pres, AUX=+] -> 'can' means that can receives the value pres for TENSE and + or true for AUX. There is a widely adopted convention which abbreviates the representation of boolean features f; instead of AUX=+ or AUX=-, we use +AUX and -AUX respectively. These are just abbreviations, however, and the parser interprets them as though + and - are like any other atomic value. \n",
    "```\n",
    "V[TENSE=pres, +AUX] -> 'can'\n",
    "V[TENSE=pres, +AUX] -> 'may'\n",
    "\n",
    "V[TENSE=pres, -AUX] -> 'walks'\n",
    "V[TENSE=pres, -AUX] -> 'likes'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have spoken of attaching \"feature annotations\" to syntactic categories. A more radical approach represents the whole category — that is, the non-terminal symbol plus the annotation — as a bundle of features. For example, N[NUM=sg] contains part of speech information which can be represented as POS=N. An alternative notation for this category therefore is [POS=N, NUM=sg].\n",
    "\n",
    "In addition to atomic-valued features, features may take values that are themselves feature structures. For example, we can group together agreement features (e.g., person, number and gender) as a distinguished part of a category, grouped together as the value of *AGR*. In this case, we say that AGR has a **complex** value. Below depicts the structure, in a format known as an **attribute value matrix** (AVM).\n",
    "```\n",
    "[POS = N           ]\n",
    "[                  ]\n",
    "[AGR = [PER = 3   ]]\n",
    "[      [NUM = pl  ]]\n",
    "[      [GND = fem ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the possibility of using features like AGR, we can refactor a grammar like 1.1 so that agreement features are bundled together. A tiny grammar illustrating this idea is shown in \n",
    "```\n",
    "S                    -> NP[AGR=?n] VP[AGR=?n]\n",
    "NP[AGR=?n]           -> PropN[AGR=?n]\n",
    "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
    "\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
    "PropN[AGR=[NUM=sg, PER=3]]            -> 'Kim'\n",
    "Adj                                   -> 'happy'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Feature Structures\n",
    "Feature structures in NLTK are declared with the FeatStruct() constructor. Atomic feature values can be strings or integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(TENSE='past',NUM='sg')\n",
    "print(fs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature structure is actually just a kind of dictionary, and so we access its values by indexing in the usual way. We can use our familiar syntax to assign values to features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fem\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(PER=3,NUM='pl',GND='fem')\n",
    "print(fs1['GND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CASE='acc', GND='fem', NUM='pl', PER=3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs1['CASE']='acc'\n",
    "fs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n"
     ]
    }
   ],
   "source": [
    "fs2=nltk.FeatStruct(POS='N',AGR=fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR']['PER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method of specifying feature structures is to use a bracketed string consisting of feature-value pairs in the format feature=value, where values may themselves be feature structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 2     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"[POS='N',AGR=[PER=2,NUM='pl',GND='fem']]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature structures are not inherently tied to linguistic objects; they are general purpose structures for representing knowledge. For example, we could encode information about a person in a feature structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ AGE   = 33               ]\n",
      "[ NAME  = 'Lee'            ]\n",
      "[ TELNO = '01 27 86 42 96' ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(NAME='Lee',TELNO='01 27 86 42 96',AGE=33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed acyclic graphs\n",
    "![dcg](http://www.nltk.org/images/dag01.png)\n",
    "![dcg2](http://www.nltk.org/images/dag02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature path is a sequence of arcs that can be followed from the root node. We will represent paths as tuples. Thus, ('ADDRESS', 'STREET') is a feature path whose value in above graph is the node labeled 'rue Pascal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dcg3](http://www.nltk.org/images/dag03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph,he value of the path ('ADDRESS') is identical to the value of the path ('SPOUSE', 'ADDRESS'). DAGs such as above are said to involve **structure sharing** or **reentrancy**. When two paths have the same value, they are said to be **equivalent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to indicate reentrancy in our matrix-style representations, we will prefix the first occurrence of a shared feature structure with an integer in parentheses, such as (1). Any later reference to that structure will use the notation ->(1), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"\"\"[NAME='Lee',ADDRESS=(1)[NUMBER=74,STREET='rue Pascal'],\n",
    "                        SPOUSE=[NAME='Kim',ADDRESS->(1)]]\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bracketed integer is sometimes called a **tag** or a **coindex**. The choice of integer is not significant. There can be any number of tags within a single feature structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsumption and Unification\n",
    "It is standard to think of feature structures as providing partial information about some object, in the sense that we can order feature structures according to how much information they contain. For example, (a) has less information than (b), which in turn has less information than (c)\n",
    "```\n",
    "a.\t\t\n",
    "[NUMBER = 74]\n",
    "\n",
    "b.\t\t\n",
    "[NUMBER = 74          ]\n",
    "[STREET = 'rue Pascal']\n",
    "\n",
    "c.\t\t\n",
    "[NUMBER = 74          ]\n",
    "[STREET = 'rue Pascal']\n",
    "[CITY = 'Paris'       ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ordering is called **subsumption**; FS0 subsumes FS1 if all the information contained in FS0 is also contained in FS1. We use the symbol ⊑ to represent subsumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging information from two feature structures is called **unification** and is supported by the `unify()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(NUMBER=74,STREET='rue Pascal')\n",
    "fs2 = nltk.FeatStruct(CITY='Paris')\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ![fs1](http://www.nltk.org/images/dag04-1.png)\n",
    "b. ![fs2](http://www.nltk.org/images/dag04-2.png)\n",
    "c. ![fsMerge](http://www.nltk.org/images/dag04-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unification is formally defined as a (partial) binary operation: FS0 ⊔ FS1. Unification is symmetric, so FS0 ⊔ FS1 = FS1 ⊔ FS0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If FS0 ⊑ FS1, then FS0 ⊔ FS1 = FS1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unification between FS0 and FS1 will fail if the two feature structures share a path π, but the value of π in FS0 is a distinct atom from the value of π in FS1. This is implemented by setting the result of unification to be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "fs0 = nltk.FeatStruct(A='a')\n",
    "fs1 = nltk.FeatStruct(A='b')\n",
    "fs2=fs0.unify(fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee,\n",
    "                          ADDRESS=[NUMBER=74,\n",
    "                                   STREET='rue Pascal'],\n",
    "                          SPOUSE= [NAME=Kim,\n",
    "                                   ADDRESS=[NUMBER=74,\n",
    "                                             STREET='rue Pascal']]]\"\"\")\n",
    "print(fs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we augment Kim's address with a specification for CITY? Notice that fs1 needs to include the whole path from the root of the feature structure down to CITY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [           [ CITY   = 'Paris'      ] ] ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(\"[SPOUSE=[ADDRESS=[CITY=Paris]]]\")\n",
    "print(fs1.unify(fs0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               [ CITY   = 'Paris'      ] ]\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
    "                          SPOUSE=[NAME=Kim, ADDRESS->(1)]]\"\"\")\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just updating what was in effect Kim's \"copy\" of Lee's address, we have now updated both their addresses at the same time. More generally, if a unification adds information to the value of some path π, then that unification simultaneously updates the value of any path that is equivalent to π.\n",
    "\n",
    "As we have already seen, structure sharing can also be stated using variables such as ?x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS1 = ?x ]\n",
      "[ ADDRESS2 = ?x ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(\"[ADDRESS1=[NUMBER=74,STREET='rue Pascal']]\")\n",
    "fs2 = nltk.FeatStruct(\"[ADDRESS1=?x,ADDRESS2=?x]\")\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\n",
      "[                [ STREET = 'rue Pascal' ] ]\n",
      "[                                          ]\n",
      "[ ADDRESS2 -> (1)                          ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2.unify(fs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending a Feature based Grammar\n",
    "## Subcategorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV and TV for intransitive and transitive verbs\n",
    "```\n",
    "VP -> IV\n",
    "VP -> TV NP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalized Phrase Structure Grammar** (GPSG) allows lexical categories to bear a  SUBCAT which tells us what subcategorization class the item belongs to. While GPSG used integer values for SUBCAT, the example below adopts more mnemonic values, namely intrans,  trans and clause:\n",
    "```\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
    "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
    "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
    "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
    "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=past, NUM=?n] -> 'disappeared' | 'walked'\n",
    "V[SUBCAT=trans, TENSE=past, NUM=?n] -> 'saw' | 'liked'\n",
    "V[SUBCAT=clause, TENSE=past, NUM=?n] -> 'said' | 'claimed'\n",
    "```\n",
    "\n",
    "When we see a lexical category like V[SUBCAT=trans], we can interpret the SUBCAT specification as a pointer to a production in which V[SUBCAT=trans] is introduced as the head child in a VP production. By convention, there is a correspondence between the values of SUBCAT and the productions that introduce lexical heads. On this approach, SUBCAT can only appear on lexical categories; it makes no sense, for example, to specify a SUBCAT value on VP. As required, walk and like both belong to the category V. Nevertheless, walk will only occur in VPs expanded by a production with the feature SUBCAT=intrans on the right hand side, as opposed to like, which requires a SUBCAT=trans.\n",
    "\n",
    "SBar is a label for subordinate clauses\n",
    "\n",
    "```\n",
    "SBar -> Comp S\n",
    "Comp -> 'that'\n",
    "```\n",
    "\n",
    "The resulting structure is as follows:\n",
    "![subcat_struc](http://www.nltk.org/book/tree_images/ch09-tree-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative treatment of subcategorization, due originally to a framework known as categorial grammar, is represented in feature based frameworks such as PATR and Head-driven Phrase Structure Grammar. Rather than using SUBCAT values as a way of indexing productions, the SUBCAT value directly encodes the valency of a head (the list of arguments that it can combine with). For example, a verb like put that takes NP and PP complements (put the book on the table) might be represented as:\n",
    "```\n",
    "V[SUBCAT=<NP, NP, PP>]\n",
    "```\n",
    "\n",
    "This says that the verb can combine with three arguments. The leftmost element in the list is the subject NP, while everything else — an NP followed by a PP in this case — comprises the subcategorized-for complements. When a verb like put is combined with appropriate complements, the requirements which are specified in the  SUBCAT are discharged, and only a subject NP is needed. This category, which corresponds to what is traditionally thought of as VP, might be represented as follows:\n",
    "```\n",
    "V[SUBCAT=<NP>]\n",
    "```\n",
    "\n",
    "Finally, a sentence is a kind of verbal category that has no requirements for further arguments, and hence has a SUBCAT whose value is the empty list. The following tree shows how these category assignments combine in a parse of Kim put the book on the table.\n",
    "![subcat2](http://www.nltk.org/book/tree_images/ch09-tree-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heads Revisited\n",
    "We noted in the previous section that by factoring subcategorization information out of the main category label, we could express more generalizations about properties of verbs. Another property of this kind is the following: expressions of category V are heads of phrases of category VP. Similarly, Ns are heads of NPs, As (i.e., adjectives) are heads of APs, and Ps (i.e., prepositions) are heads of PPs. Not all phrases have heads — for example, it is standard to say that coordinate phrases (e.g., the book and the bell) lack heads — nevertheless, we would like our grammar formalism to express the parent / head-child relation where it holds. \n",
    "\n",
    "notion of **phraseal level**:  It is usual to recognize three such levels. If N represents the lexical level, then N' represents the next level up, corresponding to the more traditional category Nom, while N'' represents the phrasal level, corresponding to the category NP.\n",
    "\n",
    "a. ![phrasal_levela](http://www.nltk.org/book/tree_images/ch09-tree-12.png)\n",
    "b. ![phrasal_levelb](http://www.nltk.org/book/tree_images/ch09-tree-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head of the structure (a) is N while N' and N'' are called **(phrasal) projections** of N. N'' is the **maximal projection**, and N is sometimes called the **zero projection**. One of the central claims of X-bar syntax is that all constituents share a structural similarity. Using X as a variable over N, V, A and P, we say that directly subcategorized complements of a lexical head  X are always placed as siblings of the head, whereas adjuncts are placed as siblings of the intermediate category, X'. Thus, the configuration of the two P'' adjuncts as follows contrasts with that of the complement P'' in (a).\n",
    "![](http://www.nltk.org/book/tree_images/ch09-tree-14.png)\n",
    "\n",
    "The above nested structure is achieved by two applications of the recursive rule expanding N[BAR=1].\n",
    "\n",
    "```S -> N[BAR=2] V[BAR=2]\n",
    "N[BAR=2] -> Det N[BAR=1]\n",
    "N[BAR=1] -> N[BAR=1] P[BAR=2]\n",
    "N[BAR=1] -> N[BAR=0] P[BAR=2]\n",
    "N[BAR=1] -> N[BAR=0]XS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Verbs and Inversion\n",
    "Inverted clauses — where the order of subject and verb is switched — occur in English interrogatives and also after 'negative' adverbs:\n",
    "   \n",
    "(37)\t\t\n",
    "a.\t\tDo you like children?\n",
    "\n",
    "b.\t\tCan Jody walk?\n",
    "\n",
    "\n",
    "(38)\t\t\n",
    "a.\t\tRarely do you see Kim.\n",
    "\n",
    "b.\t\tNever have I seen this dog.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbs that can be positioned initially in inverted clauses belong to the class known as **auxiliaries**, and as well as do, can and have include be, will and shall. One way of capturing such structures is with the following production:\n",
    "```\n",
    "(41)\n",
    "S[+INV] -> V[+AUX] NP VP\n",
    "```\n",
    "\n",
    "*Example*:\n",
    "![](http://www.nltk.org/book/tree_images/ch09-tree-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbounded Dependency Constructions\n",
    "The verb *like* requires an NP complement, while *put* requires both a following NP and PP. These complements are obligatory: omitting them leads to ungrammaticality. Yet there are contexts in which obligatory complements can be omitted, as (45) and (46) illustrate.\n",
    "\n",
    "(45)\t\t\n",
    "a.\t\tKim knows who you like.\n",
    "\n",
    "b.\t\tThis music, you really like.\n",
    "\n",
    "\n",
    "(46)\t\t\n",
    "a.\t\tWhich card do you put into the slot?\n",
    "\n",
    "b.\t\tWhich slot do you put the card into?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, an obligatory complement can be omitted if there is an appropriate **filler** in the sentence, such as the question word who in (45a), the preposed topic this music in (45b), or the wh phrases which card/slot in (46). It is common to say that sentences like (45) – (46) contain gaps where the obligatory complements have been omitted.\n",
    "\n",
    "So, a gap can occur if it is licensed by a filler. Conversely, fillers can only occur if there is an appropriate gap elsewhere in the sentence.\n",
    "\n",
    "The mutual co-occurence between filler and gap is sometimes termed a \"dependency\". One issue of considerable importance in theoretical linguistics has been the nature of the material that can intervene between a filler and the gap that it licenses; in particular, can we simply list a finite set of sequences that separate the two? The answer is No: there is no upper bound on the distance between filler and gap.This fact can be easily illustrated with constructions involving sentential complements, as shown in (50).\n",
    "```\n",
    "(50)\t\t\n",
    "a.\t\tWho do you like __?\n",
    "\n",
    "b.\t\tWho do you claim that you like __?\n",
    "\n",
    "c.\t\tWho do you claim that Jody says that you like __?\n",
    "```\n",
    "\n",
    "Since we can have indefinitely deep recursion of sentential complements, the gap can be embedded indefinitely far inside the whole sentence. This constellation of properties leads to the notion of an unbounded dependency construction; that is, a filler-gap dependency where there is no upper bound on the distance between filler and gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of mechanisms have been suggested for handling unbounded dependencies in formal grammars; here we illustrate the approach due to Generalized Phrase Structure Grammar that involves slash categories. A slash category has the form Y/XP; we interpret this as a phrase of category Y that is missing a sub-constituent of category XP. For example, S/NP is an S that is missing an NP. The use of slash categories is illustrated in (51)\n",
    "![](http://www.nltk.org/book/tree_images/ch09-tree-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can accommodate them within our existing feature based framework, by treating slash as a feature, and the category to its right as a value; that is,  S/NP is reducible to S[SLASH=NP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n"
     ]
    }
   ],
   "source": [
    "## 3.1 Grammar with productions for inverted clauses and long-distance dependencies, making use of slash categories\n",
    "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'who do you claim that you like'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more readable version of this tree:\n",
    "![](http://www.nltk.org/book/tree_images/ch09-tree-17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[-WH] you)\n",
      "  (VP[]\n",
      "    (V[-AUX, SUBCAT='clause'] claim)\n",
      "    (SBar[]\n",
      "      (Comp[] that)\n",
      "      (S[-INV]\n",
      "        (NP[-WH] you)\n",
      "        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n"
     ]
    }
   ],
   "source": [
    "## The grammar in 3.1 will also allow us to parse sentences without gaps:\n",
    "tokens = 'you claim that you like cats'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (Adv[+NEG] rarely)\n",
      "  (S[+INV]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\n"
     ]
    }
   ],
   "source": [
    "## In addition, it admits inverted sentences which do not involve wh constructions:\n",
    "tokens = 'rarely do you sing'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case and Gender in German\n",
    "Compared with English, German has a relatively rich morphology for agreement. For example, the definite article in German varies with case, gender and number, as shown in 3.1.\n",
    "\n",
    "Table 3.1:\n",
    "\n",
    "Morphological Paradigm for the German definite Article\n",
    "\n",
    "|Case|Masc|Fem|Neut|Plural|\n",
    "|----|----|---|----|------|\n",
    "|Nom|der|die|das|die|\n",
    "|Gen|des|der|des|der|\n",
    "|Dat|dem|der|dem|den|\n",
    "|Acc|den|die|das|die|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Grammar Productions\n",
      "S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
      "VP[AGR=?a] -> IV[AGR=?a]\n",
      "VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
      "# Lexical Productions\n",
      "# Singular determiners\n",
      "# masc\n",
      "Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der' \n",
      "Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
      "Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
      "# fem\n",
      "Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
      "Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "# Plural determiners\n",
      "Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den' \n",
      "Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "# Nouns\n",
      "N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
      "N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
      "N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
      "N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
      "# Pronouns\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
      "# Verbs\n",
      "IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
      "IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
      "IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
      "IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n"
     ]
    }
   ],
   "source": [
    "## The grammar in 3.2 illustrates the interaction of agreement (comprising person, number and gender) with case\n",
    "nltk.data.show_cfg('grammars/book_grammars/german.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'ich folge den Katzen'.split()\n",
    "cp = load_parser('grammars/book_grammars/german.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing grammars, excluding ungrammatical word sequences is often as challenging as parsing grammatical ones. In order to get an idea where and why a sequence fails to parse, setting the trace parameter of the load_parser() method can be crucial. Consider the following parse failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.ich.fol.den.Kat.|\n",
      "Leaf Init Rule:\n",
      "|[---]   .   .   .| [0:1] 'ich'\n",
      "|.   [---]   .   .| [1:2] 'folge'\n",
      "|.   .   [---]   .| [2:3] 'den'\n",
      "|.   .   .   [---]| [3:4] 'Katze'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] PRO[AGR=[NUM='sg', PER=1], CASE='nom'] -> 'ich' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] NP[AGR=[NUM='sg', PER=1], CASE='nom'] -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a] {?a: [NUM='sg', PER=1]}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [---]   .   .| [1:2] TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] -> 'folge' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [--->   .   .| [1:2] VP[AGR=?a] -> TV[AGR=?a, OBJCASE=?c] * NP[CASE=?c] {?a: [NUM='sg', PER=1], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] -> 'den' *\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[NUM='pl', PER=3], CASE='dat'] -> 'den' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [NUM='pl', PER=3], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [GND='masc', NUM='sg', PER=3], ?c: 'acc'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   .   [---]| [3:4] N[AGR=[GND='fem', NUM='sg', PER=3]] -> 'Katze' *\n"
     ]
    }
   ],
   "source": [
    "tokens = 'ich folge den Katze'.split()\n",
    "cp = load_parser('grammars/book_grammars/german.fcfg', trace=2)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- The traditional categories of context-free grammar are atomic symbols. An important motivation for feature structures is to capture fine-grained distinctions that would otherwise require a massive multiplication of atomic categories.\n",
    "- By using variables over feature values, we can express constraints in grammar productions that allow the realization of different feature specifications to be inter-dependent.\n",
    "- Typically we specify fixed values of features at the lexical level and constrain the values of features in phrases to unify with the corresponding values in their children.\n",
    "- Feature values are either atomic or complex. A particular sub-case of atomic value is the Boolean value, represented by convention as [+/- f].\n",
    "- Two features can share a value (either atomic or complex). Structures with shared values are said to be re-entrant. Shared values are represented by numerical indexes (or tags) in AVMs.\n",
    "- A path in a feature structure is a tuple of features corresponding to the labels on a sequence of arcs from the root of the graph representation.\n",
    "- Two paths are equivalent if they share a value.\n",
    "- Feature structures are partially ordered by subsumption. FS0 subsumes FS1 when all the information contained in FS0 is also present in FS1.\n",
    "- The unification of two structures FS0 and FS1, if successful, is the feature structure FS2 that contains the combined information of both FS0 and FS1.\n",
    "- If unification adds information to a path π in FS, then it also adds information to every path π' equivalent to π.\n",
    "- We can use feature structures to build succinct analyses of a wide variety of linguistic phenomena, including verb subcategorization, inversion constructions, unbounded dependency constructions and case government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
